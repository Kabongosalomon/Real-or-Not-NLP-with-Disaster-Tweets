{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "fastai-with-transformers-bert-roberta.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kabongosalomon/Real-or-Not-NLP-with-Disaster-Tweets/blob/master/fastai-with-transformers-bert-roberta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAbQ88Tt479Z",
        "colab_type": "text"
      },
      "source": [
        "# Fastai with HuggingFace ü§óTransformers (BERT, RoBERTa, XLNet, XLM, DistilBERT)\n",
        "\n",
        "![fastai + Transformers](https://i.ibb.co/qspmrcm/fastai-transformers-1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5QWHHq2479c",
        "colab_type": "text"
      },
      "source": [
        "N.B. This implementation is a supplement of the Medium article [\"Fastai with ü§óTransformers (BERT, RoBERTa, XLNet, XLM, DistilBERT)\"](https://medium.com/p/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2?source=email-29c8f5cf1dc4--writer.postDistributed&sk=119c3e5d748b2827af3ea863faae6376).\n",
        "\n",
        "**Also, remember the upvote button is next to the fork button, and it's free too!** üòâ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaEmbT8z479d",
        "colab_type": "text"
      },
      "source": [
        "## Introduction : Story of transfer learning in NLP\n",
        "In early 2018, Jeremy Howard (co-founder of fast.ai) and Sebastian Ruder introduced the  [Universal Language Model Fine-tuning for Text Classification](https://medium.com/r/?url=https%3A%2F%2Farxiv.org%2Fpdf%2F1801.06146.pdf) (ULMFiT) method. ULMFiT was the first **Transfer Learning** method applied to NLP. As a result, besides significantly outperforming many state-of-the-art tasks, it allowed, with only 100 labeled examples, to match performances equivalent to models trained on 100√ó  more data.\n",
        "\n",
        "The first time I heard about ULMFiT was during a [fast.ai course](https://course.fast.ai/videos/?lesson=4) given by Jeremy Howard. He demonstrated how it was easy ‚Ää-‚Ää thanks to the ``fastai`` library ‚Ää-‚Ää to implement the complete ULMFit method with only a few lines of codes. In his demo, he used an AWD-LSTM neural network pre-trained on Wikitext-103 and get rapidly state-of-the-art results. He also explained key techniques - also demonstrated in ULMFiT - to fine-tune the models like **Discriminate Learning Rate**, **Gradual Unfreezing** or **Slanted Triangular Learning Rates**.\n",
        "\n",
        "Since the introduction of ULMFiT, **Transfer Learning** became very popular in NLP and yet Google (BERT, Transformer-XL, XLNet), Facebook (RoBERTa, XLM) or even OpenAI (GPT, GPT-2) begin to pre-train their own model on very large corpora. This time, instead of using the AWD-LSTM neural network, they all used a more powerful architecture based on the Transformer (cf. [Attention is all you need](https://arxiv.org/abs/1706.03762)).\n",
        "\n",
        "Although these models are powerful, ``fastai`` do not integrate all of them. Fortunately, [HuggingFace](https://huggingface.co/) ü§ó created the well know [transformers library](https://github.com/huggingface/transformers). Formerly knew as ``pytorch-transformers`` or ``pytorch-pretrained-bert``, this library brings together over 40 state-of-the-art pre-trained NLP models (BERT, GPT-2, RoBERTa, CTRL‚Ä¶). The implementation gives interesting additional utilities like tokenizer, optimizer or scheduler.\n",
        "\n",
        "The ``transformers`` library can be self-sufficient but incorporating it within the ``fastai`` library provides simpler implementation compatible with powerful fastai tools like  **Discriminate Learning Rate**, **Gradual Unfreezing** or **Slanted Triangular Learning Rates**. The point here is to allow anyone ‚Äî expert or non-expert ‚Äî to get easily state-of-the-art results and to ‚Äúmake NLP uncool again‚Äù.\n",
        "\n",
        "It worth noting that the integration of the HuggingFace ``transformers`` library in ``fastai`` has already been demonstrated in:\n",
        "* Keita Kurita's article [A Tutorial to Fine-Tuning BERT with Fast AI](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/) which makes ``pytorch_pretrained_bert`` library compatible with ``fastai``.\n",
        "* Dev Sharma's article [Using RoBERTa with Fastai for NLP](https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c) which makes ``pytorch_transformers`` library compatible with ``fastai``.\n",
        "\n",
        "Although these articles are of high quality, some part of their demonstration is not anymore compatible with the last version of ``transformers``.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3uCMikT479f",
        "colab_type": "text"
      },
      "source": [
        "## üõ† Integrating transformers with fastai for multiclass classification\n",
        "Before beginning the implementation, note that integrating ``transformers`` within ``fastai`` can be done in multiple different ways. For that reason, I decided to bring simple solutions, that are the most generic and flexible. More precisely, I try to make the minimum of modification in both libraries while making them compatible with the maximum amount of transformer architectures.\n",
        "\n",
        "Note that in addition to this NoteBook and the [Medium article](https://medium.com/p/fastai-with-transformers-bert-roberta-xlnet-xlm-distilbert-4f41ee18ecb2?source=email-29c8f5cf1dc4--writer.postDistributed&sk=119c3e5d748b2827af3ea863faae6376), I made another version available on my GitHub(TODO add link)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ReZVr93479g",
        "colab_type": "text"
      },
      "source": [
        "### Libraries Installation\n",
        "Before starting the implementation, you will need to install the ``fastai`` and ``transformers`` libraries. To do so, just follow the instructions [here](https://github.com/fastai/fastai/blob/master/README.md#installation) and [here](https://github.com/huggingface/transformers#installation).\n",
        "\n",
        "In Kaggle, the ``fastai`` library is already installed. So you just have to instal ``transformers`` with :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEKp9HTG479i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "5d85ff6a-b41f-4329-ad44-f250fe40cb28"
      },
      "source": [
        "%%bash\n",
        "pip install transformers"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.35)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.40)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "ut3mUFGA479n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from pathlib import Path \n",
        "\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "import random \n",
        "\n",
        "# fastai\n",
        "from fastai import *\n",
        "from fastai.text import *\n",
        "from fastai.callbacks import *\n",
        "\n",
        "# transformers\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
        "from transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\n",
        "from transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuVt3M0h479r",
        "colab_type": "text"
      },
      "source": [
        "The current versions of the fastai and transformers libraries are respectively 1.0.58 and 2.1.1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsHAhr6N479s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "bc8886e8-bec2-4966-cfb1-322d4e2197b5"
      },
      "source": [
        "import fastai\n",
        "import transformers\n",
        "print('fastai version :', fastai.__version__)\n",
        "print('transformers version :', transformers.__version__)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fastai version : 1.0.59\n",
            "transformers version : 2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF0q2miF479w",
        "colab_type": "code",
        "colab": {},
        "outputId": "8f944535-acc4-4467-b70c-c2d0023ee5a7"
      },
      "source": [
        "import fastai\n",
        "import transformers\n",
        "print('fastai version :', fastai.__version__)\n",
        "print('transformers version :', transformers.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fastai version : 1.0.58\n",
            "transformers version : 2.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0t7I0T8479z",
        "colab_type": "text"
      },
      "source": [
        "### üé¨ The example¬†task\n",
        "The chosen task is a multi-class text classification on [Movie Reviews](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/overview).\n",
        "\n",
        "For each text movie review, the model has to predict a label for the sentiment. We evaluate the outputs of the model on classification accuracy. The sentiment labels are:\n",
        "* 0 ‚Üí Negative\n",
        "* 1 ‚Üí Somewhat negative\n",
        "* 2 ‚Üí Neutral\n",
        "* 3 ‚Üí Somewhat positive\n",
        "* 4 ‚Üí Positive\n",
        "\n",
        "The data is loaded into a ``DataFrame`` using ``pandas``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90BEEzW04790",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "#     for filename in filenames:\n",
        "#         print(os.path.join(dirname, filename))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "P3mSnA8_4794",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "dc6810a9-9448-473f-d343-f312961d0ec8"
      },
      "source": [
        "# DATA_ROOT = Path(\"..\") / \"/kaggle/input/sentiment-analysis-on-movie-reviews\"\n",
        "# train = pd.read_csv(DATA_ROOT / 'train.tsv.zip', sep=\"\\t\")\n",
        "# test = pd.read_csv(DATA_ROOT / 'test.tsv.zip', sep=\"\\t\")\n",
        "\n",
        "# train = pd.read_csv(\"./data/train.csv\")\n",
        "# test = pd.read_csv(\"./data/test.csv\")\n",
        "\n",
        "train = pd.read_csv(\"./train.csv\")\n",
        "test = pd.read_csv(\"./test.csv\")\n",
        "\n",
        "print(train.shape,test.shape)\n",
        "train.head()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7613, 5) (3263, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ...                                               text target\n",
              "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TICWxr04798",
        "colab_type": "text"
      },
      "source": [
        "It is worth noting that in the dataset there are no individual movie reviews but rather phrases taken out of context and split into smaller parts, each with an assigned sentiment label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNYdH5Eu4799",
        "colab_type": "text"
      },
      "source": [
        "### Main transformers classes\n",
        "In ``transformers``, each model architecture is associated with 3 main types of classes:\n",
        "* A **model class** to load/store a particular pre-train model.\n",
        "* A **tokenizer class** to pre-process the data and make it compatible with a particular model.\n",
        "* A **configuration class** to load/store the configuration of a particular model.\n",
        "\n",
        "For example, if you want to use the Bert architecture for text classification, you would use [``BertForSequenceClassification``](https://huggingface.co/transformers/model_doc/bert.html#bertforsequenceclassification) for the **model class**, [``BertTokenizer``](https://huggingface.co/transformers/model_doc/bert.html#berttokenizer) for the **tokenizer class** and [``BertConfig``](https://huggingface.co/transformers/model_doc/bert.html#bertconfig) for the **configuration class**.¬†\n",
        "\n",
        "In order to switch easily between classes ‚Ää-‚Ää each related to a specific model type ‚Ää-‚Ää I created a dictionary that allows loading the correct classes by just specifying the correct model type name."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTGKLXEA479-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_CLASSES = {\n",
        "    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n",
        "    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n",
        "    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n",
        "    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n",
        "    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kidf_bdO47-B",
        "colab_type": "text"
      },
      "source": [
        "You will see later, that those classes share a common class method ``from_pretrained(pretrained_model_name,¬†...)``. In our case, the parameter ``pretrained_model_name`` is a string with the shortcut name of a pre-trained model/tokenizer/configuration to load, e.g ``'bert-base-uncased'``. We can find all the shortcut names in the transformers documentation [here](https://huggingface.co/transformers/pretrained_models.html#pretrained-models)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u27V1en147-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parameters\n",
        "seed = 42\n",
        "use_fp16 = False\n",
        "bs = 16\n",
        "\n",
        "model_type = 'roberta'\n",
        "pretrained_model_name = 'roberta-base'\n",
        "\n",
        "# model_type = 'bert'\n",
        "# pretrained_model_name='bert-base-uncased'\n",
        "\n",
        "# model_type = 'distilbert'\n",
        "# pretrained_model_name = 'distilbert-base-uncased'\n",
        "\n",
        "#model_type = 'xlm'\n",
        "#pretrained_model_name = 'xlm-clm-enfr-1024'\n",
        "\n",
        "#model_type = 'xlnet'\n",
        "#pretrained_model_name = 'xlnet-base-cased'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lQT6lYA47-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzEuSXEv47-J",
        "colab_type": "text"
      },
      "source": [
        "Print the available values for ``pretrained_model_name`` (shortcut names) corresponding to the ``model_type`` used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ht-l0OF347-K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "92bec273-0139-44b4-cb0d-782857078f81"
      },
      "source": [
        "model_class.pretrained_model_archive_map.keys()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['roberta-base', 'roberta-large', 'roberta-large-mnli', 'distilroberta-base', 'roberta-base-openai-detector', 'roberta-large-openai-detector'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHmMk0vr47-P",
        "colab_type": "text"
      },
      "source": [
        "It is worth noting that in this case, we use the ``transformers`` library only for a multi-class text classification task. For that reason, this tutorial integrates only the transformer architectures that have a model for sequence classification implemented. These model types are¬†:\n",
        "* BERT (from Google)\n",
        "* XLNet (from Google/CMU)\n",
        "* XLM (from Facebook)\n",
        "* RoBERTa (from Facebook)\n",
        "* DistilBERT (from HuggingFace)\n",
        "\n",
        "However, if you want to go further‚Ää-‚Ääby implementing another type of model or NLP task‚Ää-‚Ääthis tutorial still an excellent starter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXHCAS7947-Q",
        "colab_type": "text"
      },
      "source": [
        "### Util function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpIcSMyu47-R",
        "colab_type": "text"
      },
      "source": [
        "Function to set the seed for generating random numbers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCiR0oYy47-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def seed_all(seed_value):\n",
        "    random.seed(seed_value) # Python\n",
        "    np.random.seed(seed_value) # cpu vars\n",
        "    torch.manual_seed(seed_value) # cpu  vars\n",
        "    \n",
        "    if torch.cuda.is_available(): \n",
        "        torch.cuda.manual_seed(seed_value)\n",
        "        torch.cuda.manual_seed_all(seed_value) # gpu vars\n",
        "        torch.backends.cudnn.deterministic = True  #needed\n",
        "        torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzO09PfS47-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_all(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMS0LBA147-Y",
        "colab_type": "text"
      },
      "source": [
        "### Data pre-processing\n",
        "\n",
        "To match pre-training, we have to format the model input sequence in a specific format.\n",
        "To do so, you have to first **tokenize** and then **numericalize** the texts correctly.\n",
        "The difficulty here is that each pre-trained model, that we will fine-tune, requires exactly the same specific pre-process‚Ää-‚Ää**tokenization** & **numericalization**‚Ää-‚Ääthan the pre-process used during the pre-train part.\n",
        "Fortunately, the **tokenizer class** from ``transformers`` provides the correct pre-process tools that correspond to each pre-trained model.\n",
        "\n",
        "In the ``fastai`` library, data pre-processing is done automatically during the creation of the ``DataBunch``. \n",
        "As you will see in the ``DataBunch`` implementation, the **tokenizer** and **numericalizer** are passed in the processor argument under the following format :\n",
        "\n",
        "``processor = [TokenizeProcessor(tokenizer=tokenizer,...), NumericalizeProcessor(vocab=vocab,...)]``\n",
        "\n",
        "Let's first analyse how we can integrate the ``transformers`` **tokenizer** within the ``TokenizeProcessor`` function.\n",
        "\n",
        "#### Custom Tokenizer\n",
        "This part can be a little bit confusing because a lot of classes are wrapped in each other and with similar names.\n",
        "To resume, if we look attentively at the ``fastai`` implementation, we notice that¬†:\n",
        "1. The [``TokenizeProcessor`` object](https://docs.fast.ai/text.data.html#TokenizeProcessor) takes as ``tokenizer`` argument a ``Tokenizer`` object.\n",
        "2. The [``Tokenizer`` object](https://docs.fast.ai/text.transform.html#Tokenizer) takes as ``tok_func`` argument a ``BaseTokenizer`` object.\n",
        "3. The [``BaseTokenizer`` object](https://docs.fast.ai/text.transform.html#BaseTokenizer) implement the function ``tokenizer(t:str) ‚Üí List[str]`` that take a text ``t`` and returns the list of its tokens.\n",
        "\n",
        "Therefore, we can simply create a new class ``TransformersBaseTokenizer`` that inherits from ``BaseTokenizer`` and overwrite a new ``tokenizer`` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2_XfBLg47-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformersBaseTokenizer(BaseTokenizer):\n",
        "    \"\"\"Wrapper around PreTrainedTokenizer to be compatible with fast.ai\"\"\"\n",
        "    def __init__(self, pretrained_tokenizer: PreTrainedTokenizer, model_type = 'bert', **kwargs):\n",
        "        self._pretrained_tokenizer = pretrained_tokenizer\n",
        "        self.max_seq_len = pretrained_tokenizer.max_len\n",
        "        self.model_type = model_type\n",
        "\n",
        "    def __call__(self, *args, **kwargs): \n",
        "        return self\n",
        "\n",
        "    def tokenizer(self, t:str) -> List[str]:\n",
        "        \"\"\"Limits the maximum sequence length and add the spesial tokens\"\"\"\n",
        "        CLS = self._pretrained_tokenizer.cls_token\n",
        "        SEP = self._pretrained_tokenizer.sep_token\n",
        "        if self.model_type in ['roberta']:\n",
        "            tokens = self._pretrained_tokenizer.tokenize(t, add_prefix_space=True)[:self.max_seq_len - 2]\n",
        "        else:\n",
        "            tokens = self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2]\n",
        "        return [CLS] + tokens + [SEP]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seD7bIY347-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer_tokenizer = tokenizer_class.from_pretrained(pretrained_model_name)\n",
        "transformer_base_tokenizer = TransformersBaseTokenizer(pretrained_tokenizer = transformer_tokenizer, model_type = model_type)\n",
        "fastai_tokenizer = Tokenizer(tok_func = transformer_base_tokenizer, pre_rules=[], post_rules=[])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHxo6op347-f",
        "colab_type": "text"
      },
      "source": [
        "In this implementation, be carefull about 3 things :\n",
        "1. As we are not using RNN, we have to limit the sequence length to the model input size.\n",
        "2. Most of the models require special tokens placed at the beginning and end of the sequences.\n",
        "3. Some models like RoBERTa require a space to start the input string. For those models, the encoding methods should be called with ``add_prefix_space`` set to ``True``.\n",
        "\n",
        "Below, you can find the resume of each pre-process requirement for the 5 model types used in this tutorial. You can also find this information on the [HuggingFace documentation](https://huggingface.co/transformers/) in each model section.\n",
        "\n",
        "    bert:       [CLS] + tokens + [SEP] + padding\n",
        "\n",
        "    roberta:    [CLS] + prefix_space + tokens + [SEP] + padding\n",
        "    \n",
        "    distilbert: [CLS] + tokens + [SEP] + padding\n",
        "\n",
        "    xlm:        [CLS] + tokens + [SEP] + padding\n",
        "\n",
        "    xlnet:      padding + [CLS] + tokens + [SEP]\n",
        "    \n",
        "It is worth noting that we don't add padding in this part of the implementation.¬†\n",
        "As we will see later, ``fastai`` manage it automatically during the creation of the ``DataBunch``."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkE06izm47-g",
        "colab_type": "text"
      },
      "source": [
        "#### Custom Numericalizer\n",
        "\n",
        "In ``fastai``, [``NumericalizeProcessor``  object](https://docs.fast.ai/text.data.html#NumericalizeProcessor) takes as ``vocab`` argument a [``Vocab`` object](https://docs.fast.ai/text.transform.html#Vocab). \n",
        "From this analyse, we suggest two ways to adapt the fastai numericalizer:\n",
        "1. You can, like decribed in the [Dev Sharma's article](https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c) (Section *1. Setting Up the Tokenizer*), retreive the list of tokens and create a ``Vocab`` object.\n",
        "2. Create a new class ``TransformersVocab`` that inherits from ``Vocab`` and overwrite ``numericalize`` and ``textify`` functions.\n",
        "\n",
        "Even if the first solution seems to be simpler, ``Transformers`` does not provide, for all models, a straightforward way to retreive his list of tokens. \n",
        "Therefore, I implemented the second solution, which runs for each model type.\n",
        "It consists of using the functions ``convert_tokens_to_ids`` and ``convert_ids_to_tokens`` in respectively ``numericalize`` and ``textify``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE9udBxR47-h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformersVocab(Vocab):\n",
        "    def __init__(self, tokenizer: PreTrainedTokenizer):\n",
        "        super(TransformersVocab, self).__init__(itos = [])\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def numericalize(self, t:Collection[str]) -> List[int]:\n",
        "        \"Convert a list of tokens `t` to their ids.\"\n",
        "        return self.tokenizer.convert_tokens_to_ids(t)\n",
        "        #return self.tokenizer.encode(t)\n",
        "\n",
        "    def textify(self, nums:Collection[int], sep=' ') -> List[str]:\n",
        "        \"Convert a list of `nums` to their tokens.\"\n",
        "        nums = np.array(nums).tolist()\n",
        "        return sep.join(self.tokenizer.convert_ids_to_tokens(nums)) if sep is not None else self.tokenizer.convert_ids_to_tokens(nums)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w79OB84H47-k",
        "colab_type": "text"
      },
      "source": [
        "#### Custom processor\n",
        "Now that we have our custom **tokenizer** and **numericalizer**, we can create the custom **processor**. Notice we are passing the ``include_bos = False`` and ``include_eos = False`` options. This is because ``fastai`` adds its own special tokens by default which interferes with the ``[CLS]`` and ``[SEP]`` tokens added by our custom tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysiOIi-J47-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer_vocab =  TransformersVocab(tokenizer = transformer_tokenizer)\n",
        "numericalize_processor = NumericalizeProcessor(vocab=transformer_vocab)\n",
        "\n",
        "tokenize_processor = TokenizeProcessor(tokenizer=fastai_tokenizer, include_bos=False, include_eos=False)\n",
        "\n",
        "transformer_processor = [tokenize_processor, numericalize_processor]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhSier9r47-q",
        "colab_type": "text"
      },
      "source": [
        "### Setting up the Databunch\n",
        "For the DataBunch creation, you have to pay attention to set the processor argument to our new custom processor ``transformer_processor`` and manage correctly the padding.\n",
        "\n",
        "As mentioned in the HuggingFace documentation, BERT, RoBERTa, XLM and DistilBERT are models with absolute position embeddings, so it's usually advised to pad the inputs on the right rather than the left. Regarding XLNET, it is a model with relative position embeddings, therefore, you can either pad the inputs on the right or on the left."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpBIBWsB47-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pad_first = bool(model_type in ['xlnet'])\n",
        "pad_idx = transformer_tokenizer.pad_token_id"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCKn2dgP47-t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "efbebd24-dc4e-4562-d98d-67678f888938"
      },
      "source": [
        "tokens = transformer_tokenizer.tokenize('Salut c est moi, Hello it s me')\n",
        "print(tokens)\n",
        "ids = transformer_tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(ids)\n",
        "transformer_tokenizer.convert_ids_to_tokens(ids)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Sal', 'ut', 'ƒ†c', 'ƒ†est', 'ƒ†mo', 'i', ',', 'ƒ†Hello', 'ƒ†it', 'ƒ†s', 'ƒ†me']\n",
            "[18111, 1182, 740, 3304, 7458, 118, 6, 20920, 24, 579, 162]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sal', 'ut', 'ƒ†c', 'ƒ†est', 'ƒ†mo', 'i', ',', 'ƒ†Hello', 'ƒ†it', 'ƒ†s', 'ƒ†me']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKKAFpFM47-v",
        "colab_type": "text"
      },
      "source": [
        "There is multible ways to create a DataBunch, in our implementation, we use [the data block API](https://docs.fast.ai/data_block.html#The-data-block-API), which gives more flexibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDVGMh0G47-w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "outputId": "d03cf896-ffad-4015-bac8-df03427cf6c0"
      },
      "source": [
        "databunch = (TextList.from_df(train, cols='text', processor=transformer_processor)\n",
        "             .split_by_rand_pct(0.1,seed=seed)\n",
        "             .label_from_df(cols= 'target')\n",
        "             .add_test(test)\n",
        "             .databunch(bs=bs, pad_first=pad_first, pad_idx=pad_idx))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUl96to047-y",
        "colab_type": "text"
      },
      "source": [
        "Check batch and tokenizer :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XJtBcm_47-z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "outputId": "989b2841-9b71-4b0d-9701-5e047543a3e7"
      },
      "source": [
        "print('[CLS] token :', transformer_tokenizer.cls_token)\n",
        "print('[SEP] token :', transformer_tokenizer.sep_token)\n",
        "print('[PAD] token :', transformer_tokenizer.pad_token)\n",
        "databunch.show_batch()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] token : <s>\n",
            "[SEP] token : </s>\n",
            "[PAD] token : <pad>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>&lt;s&gt; ƒ†. : ƒ†. : ƒ†. : ƒ†. : ƒ†. : ƒ†. : ƒ†. : ƒ†. : ƒ†. : ƒ†. : ƒ†. : ƒ†. : ƒ†. : ƒ†. : ƒ†. : ƒ†. : ƒ†. : ƒ†. : ƒ†. : ƒ†. : ƒ†. : ƒ†RT ƒ†Dr A yes ha 4 : ƒ†# India Ko M un Tor J aw ab Do ƒä ƒä Indian ƒ†Army ƒ†ki √Ç ƒ´ √ÉƒΩ _ ƒ†http</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>&lt;s&gt; ƒ†320 ƒ†[ IR ] ƒ†IC EM O ON ƒ†[ AF T ERS H OCK ] ƒ†| ƒ†http :// t . co / e 14 EP z hot H ƒ†| ƒ†@ dj ic em oon ƒ†| ƒ†# Dub step ƒ†# T rap Music ƒ†# D n B ƒ†# ED M ƒ†# D ance ƒ†# I ces √Ç ƒ´ √ÉƒΩ _ ƒ†http :// t . co / 22 a 9 D</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>&lt;s&gt; ƒ†No ƒ†# news ƒ†of ƒ†# host ages ƒ†in ƒ†# Lib ya ƒä ƒä http :// t . co / b jj O If z Uh L ƒä ƒä # India ƒ†# terrorism ƒ†# Af rica ƒ†# AP ƒ†# TS ƒ†# N RI ƒ†# News ƒ†# TR S ƒ†# T DP ƒ†# B JP ƒ†http :// t . co / I y w Z Al L s N 4 &lt;/s&gt;</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>&lt;s&gt; ƒ†i em bot _ h fo ƒ†: ƒ†At ƒ†10 : 00 ƒ†AM ƒ†2 ƒ†N NW ƒ†H ana ƒ†[ M au i ƒ†Co ƒ†HI ] ƒ†COUNTY ƒ†OFFIC IAL ƒ†reports ƒ†CO AST AL ƒ†FL OOD ƒ†# √Ç ƒ´ √ÉƒΩ _ ƒ†http :// t . co / G g 0 d Z S v B Z 7 ) ƒ†http :// t . co / k Be 91 a RC d w &lt;/s&gt;</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>&lt;s&gt; ƒ†D rought ƒ†fuels ƒ†bush ƒ†fires ƒ†in ƒ†Jamaica ƒ†- ƒ†http :// t . co / Z D t D q Q b A HC ƒ†http :// t . co / Ps Q CN s V fg P ƒ†- ƒ†@ J ama ica Obs erver ƒ†@ c new sl ive ƒ†RE ƒ†https :// t . co / 6 Z G ef 8 J 8 B m &lt;/s&gt;</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VP7dgTRO47-1",
        "colab_type": "text"
      },
      "source": [
        "Check batch and numericalizer :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjBJp7Pl47-2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "1b757075-b980-4832-ed25-f6aa0c400e09"
      },
      "source": [
        "print('[CLS] id :', transformer_tokenizer.cls_token_id)\n",
        "print('[SEP] id :', transformer_tokenizer.sep_token_id)\n",
        "print('[PAD] id :', pad_idx)\n",
        "test_one_batch = databunch.one_batch()[0]\n",
        "print('Batch shape : ',test_one_batch.shape)\n",
        "print(test_one_batch)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[CLS] id : 0\n",
            "[SEP] id : 2\n",
            "[PAD] id : 1\n",
            "Batch shape :  torch.Size([16, 86])\n",
            "tensor([[    0,   479,    35,  ...,   306,   571,     2],\n",
            "        [    0,  6324,  9847,  ...,     1,     1,     1],\n",
            "        [    0, 21947,   646,  ...,     1,     1,     1],\n",
            "        ...,\n",
            "        [    0,   128,   170,  ...,     1,     1,     1],\n",
            "        [    0, 16945,   734,  ...,     1,     1,     1],\n",
            "        [    0,  2612,   849,  ...,     1,     1,     1]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYPBnVyj47-4",
        "colab_type": "text"
      },
      "source": [
        "### Custom model\n",
        "As mentioned [here](https://github.com/huggingface/transformers#models-always-output-tuples), every model's forward method always outputs a ``tuple`` with various elements depending on the model and the configuration parameters. In our case, we are interested to access only to the logits.¬†\n",
        "One way to access them is to create a custom model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDvZYerk47-5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# defining our model architecture \n",
        "class CustomTransformerModel(nn.Module):\n",
        "    def __init__(self, transformer_model: PreTrainedModel):\n",
        "        super(CustomTransformerModel,self).__init__()\n",
        "        self.transformer = transformer_model\n",
        "        \n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \n",
        "        #attention_mask = (input_ids!=1).type(input_ids.type()) # Test attention_mask for RoBERTa\n",
        "        \n",
        "        logits = self.transformer(input_ids,\n",
        "                                attention_mask = attention_mask)[0]   \n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBBcJbCB47-7",
        "colab_type": "text"
      },
      "source": [
        "To make our transformers adapted to multiclass classification, before loading the pre-trained model, we need to precise the number of labels. To do so, you can modify the config instance or either modify like in [Keita Kurita's article](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/) (Section: *Initializing the Learner*) the ``num_labels`` argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbeix0DW47-7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "outputId": "78cbd512-4970-4345-a98f-612d4f8821e6"
      },
      "source": [
        "config = config_class.from_pretrained(pretrained_model_name)\n",
        "config.num_labels = 2\n",
        "config.use_bfloat16 = use_fp16\n",
        "print(config)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"is_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 2,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apwHA4-E47-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer_model = model_class.from_pretrained(pretrained_model_name, config = config)\n",
        "# transformer_model = model_class.from_pretrained(pretrained_model_name, num_labels = 5)\n",
        "\n",
        "custom_transformer_model = CustomTransformerModel(transformer_model = transformer_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtTKxLSb47_C",
        "colab_type": "text"
      },
      "source": [
        "### Learner¬†: Custom Optimizer / Custom¬†Metric\n",
        "In ``pytorch-transformers``, HuggingFace had implemented two specific optimizers ‚Ää-‚Ää BertAdam and OpenAIAdam ‚Ää-‚Ää that have been replaced by a single AdamW optimizer.\n",
        "This optimizer matches Pytorch Adam optimizer Api, therefore, it becomes straightforward to integrate it within ``fastai``.\n",
        "It is worth noting that for reproducing BertAdam specific behavior, you have to set ``correct_bias = False``.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeGl5cDi47_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.callbacks import *\n",
        "from transformers import AdamW\n",
        "from functools import partial\n",
        "\n",
        "CustomAdamW = partial(AdamW, correct_bias=False)\n",
        "\n",
        "learner = Learner(databunch, \n",
        "                  custom_transformer_model, \n",
        "                  opt_func = CustomAdamW, \n",
        "                  metrics=[accuracy, error_rate])\n",
        "\n",
        "# Show graph of learner stats and metrics after each epoch.\n",
        "learner.callbacks.append(ShowGraph(learner))\n",
        "\n",
        "# Put learn in FP16 precision mode. --> Seems to not working\n",
        "if use_fp16: learner = learner.to_fp16()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mSuQF7a47_E",
        "colab_type": "text"
      },
      "source": [
        "### Discriminative Fine-tuning and Gradual unfreezing (Optional)\n",
        "To use **discriminative layer training** and **gradual unfreezing**, ``fastai`` provides one tool that allows to \"split\" the structure model into groups. An instruction to perform that \"split\" is described in the fastai documentation [here](https://docs.fast.ai/basic_train.html#Discriminative-layer-training).\n",
        "\n",
        "Unfortunately,  the model architectures are too different to create a unique generic function that can \"split\" all the model types in a convenient way. Thereby, you will have to implement a custom \"split\" for each different model architecture.\n",
        "\n",
        "For example, if we use the RobBERTa model and that we observe his architecture by making ``print(learner.model)``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5goprgB_47_F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e985bdc-7152-4e22-d089-7bc959e2f3db"
      },
      "source": [
        "print(learner.model)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CustomTransformerModel(\n",
            "  (transformer): RobertaForSequenceClassification(\n",
            "    (roberta): RobertaModel(\n",
            "      (embeddings): RobertaEmbeddings(\n",
            "        (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
            "        (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
            "        (token_type_embeddings): Embedding(1, 768)\n",
            "        (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (2): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (3): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (4): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (5): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (6): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (7): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (8): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (9): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (10): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (11): BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSelfAttention(\n",
            "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (classifier): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0EzZNRb47_H",
        "colab_type": "text"
      },
      "source": [
        "We can decide to divide the model in 14 blocks¬†:\n",
        "* 1 Embedding\n",
        "* 12 transformer\n",
        "* 1 classifier\n",
        "\n",
        "In this case, we can split our model in this way¬†:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpjFyKHc47_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For DistilBERT\n",
        "# list_layers = [learner.model.transformer.distilbert.embeddings,\n",
        "#                learner.model.transformer.distilbert.transformer.layer[0],\n",
        "#                learner.model.transformer.distilbert.transformer.layer[1],\n",
        "#                learner.model.transformer.distilbert.transformer.layer[2],\n",
        "#                learner.model.transformer.distilbert.transformer.layer[3],\n",
        "#                learner.model.transformer.distilbert.transformer.layer[4],\n",
        "#                learner.model.transformer.distilbert.transformer.layer[5],\n",
        "#                learner.model.transformer.pre_classifier]\n",
        "\n",
        "# For roberta-base\n",
        "list_layers = [learner.model.transformer.roberta.embeddings,\n",
        "              learner.model.transformer.roberta.encoder.layer[0],\n",
        "              learner.model.transformer.roberta.encoder.layer[1],\n",
        "              learner.model.transformer.roberta.encoder.layer[2],\n",
        "              learner.model.transformer.roberta.encoder.layer[3],\n",
        "              learner.model.transformer.roberta.encoder.layer[4],\n",
        "              learner.model.transformer.roberta.encoder.layer[5],\n",
        "              learner.model.transformer.roberta.encoder.layer[6],\n",
        "              learner.model.transformer.roberta.encoder.layer[7],\n",
        "              learner.model.transformer.roberta.encoder.layer[8],\n",
        "              learner.model.transformer.roberta.encoder.layer[9],\n",
        "              learner.model.transformer.roberta.encoder.layer[10],\n",
        "              learner.model.transformer.roberta.encoder.layer[11],\n",
        "              learner.model.transformer.roberta.pooler]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bybkY0pj47_J",
        "colab_type": "text"
      },
      "source": [
        "Check groups : "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgZj5sVG47_K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f16d4ca7-dd15-490c-97af-88c6ff9e1d19"
      },
      "source": [
        "learner.split(list_layers)\n",
        "num_groups = len(learner.layer_groups)\n",
        "print('Learner split in',num_groups,'groups')\n",
        "print(learner.layer_groups)\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learner split in 14 groups\n",
            "[Sequential(\n",
            "  (0): Embedding(50265, 768, padding_idx=1)\n",
            "  (1): Embedding(514, 768, padding_idx=1)\n",
            "  (2): Embedding(1, 768)\n",
            "  (3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (4): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (5): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (6): Dropout(p=0.1, inplace=False)\n",
            "  (7): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (8): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (9): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  (10): Dropout(p=0.1, inplace=False)\n",
            "), Sequential(\n",
            "  (0): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (1): Tanh()\n",
            "  (2): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (3): Dropout(p=0.1, inplace=False)\n",
            "  (4): Linear(in_features=768, out_features=2, bias=True)\n",
            ")]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agEvJ1Eu47_M",
        "colab_type": "text"
      },
      "source": [
        "Note that I didn't found any document that has studied the influence of **Discriminative Fine-tuning** and **Gradual unfreezing** or even **Slanted Triangular Learning Rates** with transformers. Therefore, using these tools does not guarantee better results. If you found any interesting documents, please let us know in the comment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3pYiZoF47_M",
        "colab_type": "text"
      },
      "source": [
        "### Train\n",
        "Now we can finally use all the fastai build-in features to train our model. Like the ULMFiT method, we will use **Slanted Triangular Learning Rates**, **Discriminate Learning Rate** and **gradually unfreeze the model**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIEh1ZK-47_N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.save('untrain')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubETMZ7S47_O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_all(seed)\n",
        "learner.load('untrain');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df-aIZbD47_Q",
        "colab_type": "text"
      },
      "source": [
        "Therefore, we first freeze all the groups but the classifier with¬†:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-GoT8ip47_S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.freeze_to(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q48jq7oa47_W",
        "colab_type": "text"
      },
      "source": [
        "We check which layer are trainable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ-G9v3e47_X",
        "colab_type": "text"
      },
      "source": [
        "learner.unfreeze()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjT-efp447_Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eb7b89ae-daaf-4972-f451-8186af175414"
      },
      "source": [
        "learner.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CustomTransformerModel\n",
              "======================================================================\n",
              "Layer (type)         Output Shape         Param #    Trainable \n",
              "======================================================================\n",
              "Embedding            [86, 768]            38,603,520 False     \n",
              "______________________________________________________________________\n",
              "Embedding            [86, 768]            394,752    False     \n",
              "______________________________________________________________________\n",
              "Embedding            [86, 768]            768        False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 86, 86]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 86, 86]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 86, 86]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 86, 86]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 86, 86]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 86, 86]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 86, 86]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 86, 86]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 86, 86]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 86, 86]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 86, 86]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "Dropout              [12, 86, 86]         0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            590,592    False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 3072]           2,362,368  False     \n",
              "______________________________________________________________________\n",
              "Linear               [86, 768]            2,360,064  False     \n",
              "______________________________________________________________________\n",
              "LayerNorm            [86, 768]            1,536      False     \n",
              "______________________________________________________________________\n",
              "Dropout              [86, 768]            0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [768]                590,592    True      \n",
              "______________________________________________________________________\n",
              "Tanh                 [768]                0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [768]                590,592    True      \n",
              "______________________________________________________________________\n",
              "Dropout              [768]                0          False     \n",
              "______________________________________________________________________\n",
              "Linear               [2]                  1,538      True      \n",
              "______________________________________________________________________\n",
              "\n",
              "Total params: 125,237,762\n",
              "Total trainable params: 1,182,722\n",
              "Total non-trainable params: 124,055,040\n",
              "Optimized with 'transformers.optimization.AdamW', correct_bias=False\n",
              "Using true weight decay as discussed in https://www.fast.ai/2018/07/02/adam-weight-decay/ \n",
              "Loss function : FlattenedLoss\n",
              "======================================================================\n",
              "Callbacks functions applied \n",
              "    ShowGraph"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3B7JBoc47_b",
        "colab_type": "text"
      },
      "source": [
        "For **Slanted Triangular Learning Rates** you have to use the function ``one_cycle``. For more information please check the fastai documentation [here](https://docs.fast.ai/callbacks.one_cycle.html).¬†\n",
        "\n",
        "To use our ``one_cycle`` we will need an optimum learning rate. We can find this learning rate by using a learning rate finder which can be called by using ``lr_find``."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ-jbebS47_c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "f24f78c9-0e2d-499f-86d6-5dc3a717e804"
      },
      "source": [
        "learner.lr_find()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "LR Finder is complete, type {learner_name}.recorder.plot() to see the graph.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTcXVUEq47_e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "0347c295-2bcd-4233-eb85-baac5741e748"
      },
      "source": [
        "learner.recorder.plot(skip_end=10,suggestion=True)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Min numerical gradient: 6.92E-04\n",
            "Min loss divided by 10: 8.32E-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXxV5bX/8c/KRBgSCCTMU5gEBAGN\noFatQ1W0Vdra9mJLq9Vq73W6t63ttdf+rJfOtS23vVpbqi0OrVxra4utilq1akUFlRkZRQhjQghk\nIsnJWb8/zg49hBMIJDvnJHzfr9d5cfbezz575XmFs7L3s5+1zd0RERFpKi3ZAYiISGpSghARkYSU\nIEREJCElCBERSUgJQkREEspIdgBtJT8/34cPH57sMEREOpS33nqr1N0LEm3rNAli+PDhLFmyJNlh\niIh0KGb2fnPbdIlJREQSCjVBmNl0M1trZhvM7PYE24ea2Ytm9o6ZLTezyxJsrzSz28KMU0REDhda\ngjCzdOBe4FJgPHCVmY1v0uwbwGPuPgWYCfy8yfafAE+HFaOIiDQvzDOIqcAGd9/k7nXAfGBGkzYO\n5AbvewLbGzeY2UeB94BVIcYoIiLNCDNBDAK2xi0XB+vi3QXMMrNi4CngFgAz6wH8J/DfIcYnIiJH\nkOxB6quAee4+GLgMeNjM0ogljjnuXnmknc3sBjNbYmZLSkpKwo9WROQEEuZtrtuAIXHLg4N18a4D\npgO4+yIzywbygWnAJ8zsh0AvIGpmB9z9nvid3X0uMBegqKhIZWlFRNpQmGcQi4HRZlZoZlnEBqEX\nNGmzBbgQwMzGAdlAibuf4+7D3X048D/Ad5smBxGRE8mu/Qd4bMlWotH2+1s4tATh7hHgZmAhsIbY\n3UqrzGy2mV0RNPsKcL2ZLQMeBa5xPaBCROQwjy3eytceX86dC1bSXl+Toc6kdveniA0+x6+7M+79\nauADR/mMu0IJTkSkA9ldUQvAI69vITM9jTs/Mh4zC/WYnabUhohIZ1ZaWcuovj04Z3Q+v/nHZjLT\n0/j6pWNDTRJKECIiHUBpZS0FPbpw50fGU98QZe7Lm8hMN267+KTQkoQShIhIB1BSUcvEwb0wM2Zf\nMYFIg3PvixvJSk/n3z80OpRjKkGIiHQApZV15PfIAiAtzfjuxyZS3+DMeX4dmRnGjeeNavNjKkGI\niKS4mroGKmsjFOR0ObguLc344SdOIRKN8vb7e2mIOulpbXupSQlCRCTFlVbG7mDK79HlkPXpacaP\nPzmJqNPmyQGUIEREUl5JkCAKmiQIgIz08OY7J7sWk4iIHEVpReIziLApQYiIpLjSyjqAQ8Yg2oMS\nhIhIiisJziD6BHcxtRclCBGRFFdaWUuvbplkhjjekIgShIhIimucRd3elCBERFJcSUVtuw9QgxKE\niEjKK62sJb+dB6hBCUJEJOWVVtbpEpOIiByqscxGfk773sEEShAiIimtuTIb7UEJQkQkhR2pzEbY\nlCBERFJYY5mN9p5FDSEnCDObbmZrzWyDmd2eYPtQM3vRzN4xs+Vmdlmw/iIze8vMVgT/XhBmnCIi\nqaokiZeYQqvmambpwL3ARUAxsNjMFrj76rhm3wAec/f7zGw88BQwHCgFLnf37WY2AVgIDAorVhGR\nVFVaEavD1N5lNiDcM4ipwAZ33+TudcB8YEaTNg7kBu97AtsB3P0dd98erF8FdDWz9k+fIiJJVlpZ\nS14SymxAuAliELA1brmYw88C7gJmmVkxsbOHWxJ8zpXA2+5e23SDmd1gZkvMbElJSUnbRC0ikkKS\nNYsakj9IfRUwz90HA5cBD5vZwZjM7GTgB8AXE+3s7nPdvcjdiwoKCtolYBGR9lRa2TkTxDZgSNzy\n4GBdvOuAxwDcfRGQDeQDmNlg4Angc+6+McQ4RURSVrLKbEC4CWIxMNrMCs0sC5gJLGjSZgtwIYCZ\njSOWIErMrBfwV+B2d/9HiDGKiKS0korkVHKFEBOEu0eAm4ndgbSG2N1Kq8xstpldETT7CnC9mS0D\nHgWucXcP9hsF3GlmS4NX37BiFRFJRTV1DVTVNSSlzAaEeJsrgLs/RWzwOX7dnXHvVwMfSLDft4Fv\nhxmbiEiqS2aZDUj+ILWIiDTjYJmNTjgGISIirdD4LOpONwYhIiKto0tMIiKSUDLLbIAShIhIyiqp\nPJC0MhugBCEikrJKK+qSdnkJlCBERFJWMstsgBKEiEjKKqmsTdotrqAEISKSskqTWMkVlCBERFJS\nsstsgBKEiEhKapwDkaxJcqAEISKSknYHs6iTVeoblCBERFKSziBERCSh0iQX6gMlCBGRlNRYqK93\ndw1Si4hInNLK2qSW2QAlCBGRlJTsMhugBCEikpKSPYsaQk4QZjbdzNaa2QYzuz3B9qFm9qKZvWNm\ny83ssrhtXw/2W2tml4QZp4hIqkl2HSYI8ZnUZpYO3AtcBBQDi81sQfAc6kbfAB5z9/vMbDyx51cP\nD97PBE4GBgLPm9kYd28IK14RkVSS7DIbEO4ZxFRgg7tvcvc6YD4wo0kbB3KD9z2B7cH7GcB8d691\n9/eADcHniYh0etV1EarqGjr1JaZBwNa45eJgXby7gFlmVkzs7OGWY9gXM7vBzJaY2ZKSkpK2iltE\nJKkanySXn6QnyTVK9iD1VcA8dx8MXAY8bGYtjsnd57p7kbsXFRQUhBakiEh7KqlMfpkNCHEMAtgG\nDIlbHhysi3cdMB3A3ReZWTaQ38J9RUQ6pVQoswHhnkEsBkabWaGZZREbdF7QpM0W4EIAMxsHZAMl\nQbuZZtbFzAqB0cCbIcYqIpIyGmdRJ3sMIrQzCHePmNnNwEIgHfi1u68ys9nAEndfAHwF+JWZfYnY\ngPU17u7AKjN7DFgNRICbdAeTiJwoGs8gkllmA8K9xIS7P0Vs8Dl+3Z1x71cDH2hm3+8A3wkzPhGR\nVJQKZTYg+YPUIiLSRElF8mdRgxKEiEjKKa1Mfh0mUIIQEUk5qVBmA5QgRERSTqkuMYmISFONZTZ0\nBiEiIodIlTIboAQhIpJS1u2qAGBQXtckR6IEISKSUp5euZOc7AyKhvVOdihKECIiqaK+Icrza3Zx\n0bh+ZGUk/+s5+RGIiAgAizbuYV9NPdMn9E92KIAShIhIynh65U66ZaVz7pjUeHyBEoSISApoiDrP\nrd7J+WP7kp2ZnuxwACUIEZGUsHhzGaWVdVw2YUCyQzlICUJEJAU8s3InXTLSOO+k1Li8BEoQIiJJ\nF406z6zcyQfHFNC9S6hPYTgmShAiIkm2tLicnfsPcOnE1Lh7qZEShIhIkj2zcieZ6cYFY/slO5RD\nKEGIiCSRu/P0yh18YFQ+PbtmJjucQ4SaIMxsupmtNbMNZnZ7gu1zzGxp8FpnZuVx235oZqvMbI2Z\n/czMLMxYRUSSYdX2/Wwtq+HSFJkcFy+00RAzSwfuBS4CioHFZrYgeA41AO7+pbj2twBTgvdnEXtW\n9SnB5leBDwIvhRWviEgyPLNyJ+lpxkXjUy9BhHkGMRXY4O6b3L0OmA/MOEL7q4BHg/cOZANZQBcg\nE9gVYqwiIknx9ModTCvsTe/uyS/v3VSYCWIQsDVuuThYdxgzGwYUAi8AuPsi4EVgR/Ba6O5rEux3\ng5ktMbMlJSUlbRy+iEi41u+qYGNJFZdOTJ3JcfFSZZB6JvC4uzcAmNkoYBwwmFhSucDMzmm6k7vP\ndfcidy8qKEidySUiIi3x9MqdmMElJ6fW3UuNwpyRsQ0YErc8OFiXyEzgprjljwGvu3slgJk9DZwJ\nvBJCnCIiodqyp5qX15ew/0A9FQciVAT//mPDHoqG5dE3JzvZISYUZoJYDIw2s0JiiWEm8Ommjcxs\nLJAHLIpbvQW43sy+BxixAer/CTFWEZHQfOuvq3ludWwYNSPNyMnOILdrJgN6ZvPFc0cmObrmhZYg\n3D1iZjcDC4F04NfuvsrMZgNL3H1B0HQmMN/dPW73x4ELgBXEBqyfcfcnw4pVRCRMu/cf4KyRfXjg\n6tPJzkyjo9y1H2rRD3d/Cniqybo7myzflWC/BuCLYcYmItJeyqrrGFHQg65ZqVHGu6VSZZBaRKTT\nKqusI69b6t3GejRKECIiITpQ30BVXQN9eihBiIhInLKqOoCUnAh3NEoQIiIhUoIQEZGElCBERCQh\nJQgREUloT5Ag+nTWBGFmI82sS/D+PDO71cx6hRuaiEjHV1ZVS3qakZudWg8DaomWnkH8AWgIiujN\nJVZj6XehRSUi0kmUVdWT1y2LtLSOMXs6XksTRNTdI8SK6P2vu38VSM36tCIiKaSsqpbe3Tve2QO0\nPEHUm9lVwNXAX4J1HfMnFhFpR2VVdR1ygBpaniA+T6zc9nfc/b2gQuvD4YUlItI57Kmqo0/3LskO\n47i0qFhf8BzpWwHMLA/IcfcfhBmYiEhn0OnPIMzsJTPLNbPewNvAr8zsJ+GGJiLSsUUaouyrqe/c\nCQLo6e77gY8DD7n7NOBD4YUlItLxldfU494xJ8lByxNEhpkNAD7FPwepRUTkCDryLGpoeYKYTezJ\ncBvdfbGZjQDWhxeWiEjHt6ey486ihpYPUv8e+H3c8ibgyrCCEhHpDA6eQXTAZ0FAywepB5vZE2a2\nO3j9wcwGt2C/6Wa21sw2mNntCbbPMbOlwWudmZXHbRtqZs+a2RozW21mw4/lBxMRSbay6iBBdMCn\nyUHLn0n9G2KlNT4ZLM8K1l3U3A5mlg7cG7QpBhab2YLgllkA3P1Lce1vAabEfcRDxOZdPGdmPYBo\nC2MVEUkJZcElprwOeomppWMQBe7+G3ePBK95QMFR9pkKbHD3Te5eB8wHZhyh/VXAowBmNh7IcPfn\nANy90t2rWxiriEhKKKuqJTc7g8z0jlk4u6VR7zGzWWaWHrxmAXuOss8gYGvccnGw7jBmNgwoBF4I\nVo0Bys3sj2b2jpndHZyRiIh0GHuq6ujTo2POooaWJ4hrid3iuhPYAXwCuKYN45gJPO7uDcFyBnAO\ncBtwOjAi0fHM7AYzW2JmS0pKStowHBGR1ttb3XFnUUMLE4S7v+/uV7h7gbv3dfePcvS7mLYRKwve\naHCwLpGZBJeXAsXA0uDyVAT4E3BqgrjmunuRuxcVFBztipeISPvaU1lHXgcdoIbWPVHuy0fZvhgY\nbWaFZpZFLAksaNrIzMYCecCiJvv2MrPGb/0LgNVN9xURSWVlVXUddg4EtC5BHPHpF8Ff/jcTm2C3\nBnjM3VeZ2WwzuyKu6Uxgvrt73L4NxC4v/c3MVgTH+lUrYhURaVfuHrvE1EHnQEDLb3NNxI/awP0p\n4Kkm6+5ssnxXM/s+B5zSivhERJJm/4EI9Q3eoc8gjpggzKyCxInAgK6hRCQi0gns7eB1mOAoCcLd\nc9orEBGRzmRPVceeJAetG4MQEZFmNNZh6siXmJQgRERCUFZVC3TsS0xKECIiISirqgfosM+jBiUI\nEZFQlFXVkp2ZRtesjlslSAlCRCQEe6rqOvTZAyhBiIiEoqyqY9dhAiUIEZFQKEGIiEhCHb0OEyhB\niIiEoqyqrkNPkgMlCBGRNnegvoHqugZdYhIRkUPt6QSzqEEJQkSkzZVVdvxCfaAEISLS5sqqgzOI\nDvwsCFCCEBFpc411mDry40ZBCUJEpM3tqWwcg9BMahERiVNWVUdGmpHbtTUP7Uw+JQgRkTa2tzo2\nB8LMkh1Kq4SaIMxsupmtNbMNZnZ7gu1zzGxp8FpnZuVNtueaWbGZ3RNmnCIibWlPZcefRQ1HeeRo\na5hZOnAvcBFQDCw2swXuvrqxjbt/Ka79LcCUJh/zLeDlsGIUEQlDWVVdhx+ghnDPIKYCG9x9k7vX\nAfOBGUdofxXwaOOCmZ0G9AOeDTFGEZE2V1ZVR+8OfosrhJsgBgFb45aLg3WHMbNhQCHwQrCcBvwY\nuO1IBzCzG8xsiZktKSkpaZOgRURaa08nKNQHqTNIPRN43N0bguUbgafcvfhIO7n7XHcvcveigoKC\n0IMUETmaSEOUfTX1HX4WNYQ4BgFsA4bELQ8O1iUyE7gpbvlM4BwzuxHoAWSZWaW7HzbQLSKSSvZW\nx55FrQRxZIuB0WZWSCwxzAQ+3bSRmY0F8oBFjevc/TNx268BipQcRKQjKKvqHHWYIMRLTO4eAW4G\nFgJrgMfcfZWZzTazK+KazgTmu7uHFYuISHvZE5TZ6AwJItRpfu7+FPBUk3V3Nlm+6yifMQ+Y18ah\niYiEoqyqc5TZgNQZpBYR6RT26hKTiIgk0viwoF7dMpMcSespQYiItKGyqjp6ds0kM73jf712/J9A\nROQY1NQ18OLa3Ryobzh64+PQWSbJQciD1CIiqebrf1zOn5Zup3f3LD5VNITPTBvKkN7d2uzz91bV\ndYrxB1CCEJETyNMrdvCnpdv5l6IhlNfUMffljfzy5Y1cOLYvnz1zOOeMyictrXUlusuq6hjahgkn\nmZQgROSEUFJRyx1/WsnEQT359scmkJmexvbyGn73xhbmL97C82veJL9HFuMG5DJuQC5j++cwtn8u\nI/t2p0tGeouPs6eqjslDeoX4k7QfJQgR6TCWbS3npbUlTB7ai1OH9iInu2V3Crk7dzyxgsraCD/5\n1KSDA8gDe3XltktO4pYLR/HMyp28sr6Ud3fuZ95rm6mLRAHISDP+30fGc/VZw1t0HF1iEhFJgv99\nYQPPr9kFQJrBSf1zKRqWR9HwPM47qS89uyZOGE+8s41nV+/ijsvGMbpfzmHbu2SkM2PyIGZMjhWc\njjRE2bynijU7Knh40fvcvXAtV0waSN5Rvvj310SIRL3TJAjdxSQiHcamkkrOP6mAR66bxi0XjKZP\n9yz++HYx/z5/KR+8+0UeXrSZSEP0kH22l9fwzQWrOH14HteeXdii42SkpzGqbw6XTxrItz82gaq6\nCHNf2XTU/cqqg1nUneBZEKAzCBHpIOobomwpq+bSif05e3Q+Z4/OB2J/7S8rLufuhWv5f39exSOv\nb+HOy8fzgVH5uDv/+YflRBqcH31yEunHMQA9pl8Ol58ykHn/2Mx1ZxeS36P5EhplQR2mzvA0OdAZ\nhIh0EO/vqSYSdUYW9DhkfUZ6GqcN682j15/BL2adSnV9hM/c/wY3PLSEOc+v55X1pfzXh8cxrE/3\n4z72v39oNLWRBn75941HbLensvPUYQIlCBHpIDaVVAIwokmCaGRmTJ8wgOe+9EG+eslJvLqhlJ/9\nbT3njM5n1rShrTr2yIIefHTKIB5a9D679x9ott3OYFtneNwoKEGISAexsaQKgBEFRz4TyM5M56bz\nR/Hibedx28Vj+PGnJmHWurkNALdeMJpI1Pn5S4nPIlZu28fdC9cysqA7/XJ0BiEi0m42llRSkNOF\n3Bbe2tovN5ubLxhN35zsNjn+8PzufOLUwfzuzS3s2FdzyLZ1uyr47ANvkJudycPXTSOjE9RhAiUI\nEekgNpVUMvIoZw9hu/mCUbg797644eC6zaVVzLr/DTLT0/jd9dMY2KtrEiNsW0oQIpLy3J2NJVXN\njj+0lyG9u/GpoiH83+KtFO+tZlt5DZ+5/w0iUee3X5jWqoHwVKTbXEUk5ZVV1bGvpv6wO5iS4abz\nR/H7JcV8569rWLNjP/sP1PPo9WcknIDX0YV6BmFm081srZltMLPbE2yfY2ZLg9c6MysP1k82s0Vm\ntsrMlpvZv4QZp4iktsYB6mRfYoJYeY5PTxvK0yt3sruilnmfn8qEQT2THVYoQjuDMLN04F7gIqAY\nWGxmC9x9dWMbd/9SXPtbgCnBYjXwOXdfb2YDgbfMbKG7l4cVr4ikrsZbXFPhDALgxvNH8v6eKm44\ndySnDctLdjihCfMS01Rgg7tvAjCz+cAMYHUz7a8Cvgng7usaV7r7djPbDRQAShAiJ6CNJZV0yUhL\nmQHgvjnZ/ObzU5MdRujCvMQ0CNgat1wcrDuMmQ0DCoEXEmybCmQBh918bGY3mNkSM1tSUlLSJkGL\nSOrZVFJFYX734yqVIccvVe5imgk87u6HPAPQzAYADwOfd/do053cfa67F7l7UUFBQTuFKiLtbWNJ\nZcpcXjqRhJkgtgFD4pYHB+sSmQk8Gr/CzHKBvwJ3uPvroUQoIimvNtLAlrLqo86glrYXZoJYDIw2\ns0IzyyKWBBY0bWRmY4E8YFHcuizgCeAhd388xBhFJMVt2VNN1FNngPpEElqCcPcIcDOwEFgDPObu\nq8xstpldEdd0JjDf3T1u3aeAc4Fr4m6DnRxWrCKSujYeLNKnM4j2FupEOXd/Cniqybo7myzflWC/\nR4BHwoxNRDqGfxbp0xlEe0uVQWoRkYQ2llTSPzebHl1U+KG9KUGISErbVFKly0tJogQhIikrVqRP\nt7gmixKEiKSs0so6Kg5EdAaRJEoQIpKyNqZYDaYTjRLECcDdeX3THp5ZuZNIw2ET0kVSlm5xTS7d\nFtDJvfleGT9+di1vvFcGwNDe3fi380by8VMH0SUjvVWffaC+gZfW7ubJZTvYureablnp9OiSQffg\nldMlg0lDenHB2L5kZ7buWHJi2lRSRXZmGgN7pkaRvhONEkSKqKlrYPWOfdTWR5kyNI+uWa37Qn17\ny17mPLeOV9aXUpDThbsuH8+AXl35+Ysb+PofV/DT59dzw7kjuGrqULpmpVNVG+HdnftZtX0/q7fv\nZ2NJJf1ysxnTL4cx/Xowpl8Ow/p0J+rOqxtKeXLZdp5dtYvK2gh9umdx8qCe1NRF2FZ+gKraCFW1\nESoORKhriNI9K52LT+7PFZMGcvbofDI7yfN6JXwbSyoZkd+DNBXpSwoliDbi7rjTol/kSEOU1Tv2\ns6x4HyuKy1levI/1uytpiMYmk2emG5MG9+LMkX04Y0QfTm1hwnB33nyvjF/8fSMvri2hd/cs7rhs\nHLPOGHZw/4vH9+OV9aXc8+IGZv9lNfe+uIGeXTN5b08VjXPZe3XLZFRBD5ZuLecvy3cc/PysjDS6\nZKRRcSBCTnYGl03sz+WTBnLmiD4JH9LeEI1d2npy2XaeXrmTJ97ZRq9umXxoXD9ysjOoi0Spb4hS\nF4lS1xAl0uCkmZGeZqSlGekW68/uWRn0zelC39wu9M3JpiCnC/1ys8nvkYWZvjg6s00lVZwyuHM+\njKcjsEMrXHRcRUVFvmTJknY/rrvz9MqdzH5yNbWRBs4eXcC5o/P54JgC+uZmH2xXXl3H39eV8MK7\nu/n7uhLKq+sByOuWySmDe3HK4J6cMrgXGenGG5vKeH3THlZs20dD1MlMN6YW9ubi8f25+OR+DGhy\nul1T18Cfl27jwUXvs2bHfnp1y+T6c0ZwzVnD6X6EyUWLN5fxwCvvEXVn/MBcTh7Yk5MH5jKgZ/bB\nL96q2ggbdleyblcF63dXsr+mngvH9ePcMfnHdImqLhLllfUlLFi2nZfWlhCNOlkZaWRlpJGZHvs3\nI82IutMQdaIeSzANUaeyNsK+mvrDPrMgpwvTCnszbUQfzhzRm5EFPZQwOpED9Q2Mu/MZbr1gNF+6\naEyyw+m0zOwtdy9KuE0J4vhtK6/hzj+t5G/v7mb8gFzG9s/h5fWllFbWAjC2fw5TC3vz7o4Klrxf\nRtShd/cszjupgPNO6suUIb0YnNe12S+1igP1LHl/L4s27uFva3YdLDlwyuCeXDy+H9NG9OH51buY\nv3gr+2rqGds/h2vOGs6MyYNafYkq1Ryob6CkopbdFbWUVBxgx74DLN1azhubyti5/wAA+T2yKBrW\nm8KC7gzsmc3AXl0PvnKzM5Q8Opi1Oyu45H9e5qczJzNjcsJHyUgbUIJoYw1RZ95rm/nxs2txhy9f\nNIbPf2A4GelpRKPOmp37eXldKS+vK+Gt9/cyqm8PLhzXl/PH9mXS4F7H/dCTDbsreW71Lhau2snS\nrbGH66WnGZec3I+rzxzO1MLeJ9yXoLuzpaya1zft4Y1NZby1ZS/by2uobzj093pgz2xuu+QkPjp5\nkK5ndxBPrdjBjb99m7/ccnanfeZzKlCCOA6RhiivbChlf0099Q1OfUOUSEOU2kiUPy/dzopt+zj/\npAJmz5jAkN7dmv0cdw/lS3vX/gO8+V4Zpw3LS5nHMKaKaNQpraxlW3kN28sPsL28hr8s386y4n2c\nOrQX/33FBCbqunbKu+eF9fzo2XWsnn0J3bI0XBqWIyUI9XozZv9lNQ8tej/htoKcLtzz6Sl8eOKA\no375h/UXfb/cbC6fNDCUz+7o0tKMvrnZ9M3NZsrQ2Lrrzi7k8beL+eEz73LFva8y8/Qh3HbxSfTp\n0SW5wTajIeosXLWT1dv3M2FQLpOH5NG/Z/bRd+xENpZUMbBntpJDEqnnE3h6xQ4eWvQ+V585jM+d\nNZys9NhAama6kZGeRves9IR37UjqSkszPlU0hOkT+vPT59fz4Gub+evyHdx+6TiumjokZS7N1UWi\nPPFOMb/8+yY2lVYdsq1/bjaTh/Ri8tDY3JIx/XKSFGX72FhSqRLfSaYE0cTWsmq+9oflTBrSizs+\nPJ6sDCWCziQ3O5P/95HxzDx9CN9csIr/emIFizeX8d2PTUzqwH5VbYRH39zC/a+8x879Bzh5YC73\nfvpULhjbl3d37mfp1nLe2VLO0q3lPLNqJ99/+l0+NK4v/3beKE4blpe0uMPi7mwqqeLKUzU4nUxK\nEHHqIlFufvQdAO65aoqSQyc2ul8Oj1w3jXte3MCc59exZsd+fjHrNIbnt29Jh/0H6nnotc088Op7\n7K2uZ1phb37wiVM4d3T+wbOaKUPzmDI0j89/ILZPSUUtv33jfea9tpkr73uNaYW9ufH8UYfsk0xl\nVXWUVdUxqu/x//W/u6KWytoII1vxGdJ6ShBxfvjMuyzbWs59nzn1iAPP0jmkpRm3XjiaUwb35D/+\nbymX3/Mqcz41mQ+N7xf6sffV1DPvH5t54NVN7D8Q4YKxfbnp/JadDRTkdOE/PjSGG84dwaNvbuVX\nL2/i6l+/yckDc/nRJycxbkBu6PE357UNpdw6/x3210S4b9apXDju+PryYA2mfCWIZAr1T2Qzm25m\na81sg5ndnmD7nLhnTq8zs/K4bVeb2frgdXWYcQI8v3oX97/6Hp87cxiXThwQ9uEkhZx3Ul+evPls\nhvfpzhceWsKPFq49OKu9rX5dRdIAAA6ySURBVO2rrucnz63j7B+8wJzn1zFtRB+evPlsfn3N6cd8\nqahbVgbXnV3Iy187nx9eeQolFbV84cEl7Anm4RzJ39bs4ht/WsFrG0uJtsHPGo069764gVkPvEGv\nblmM6d+Df33kLf62Zhds3Ag33gi5uZCWFvv3xhtj65vx+sY9mMGY/koQyRTaba5mlg6sAy4CioHF\nwFXuvrqZ9rcAU9z9WjPrDSwBigAH3gJOc/e9zR2vNbe5bi+v4bKfvcLAnl35441nqbDcCepAfQN3\nLVjF/MVbuWLSQP7nXya36ZyJDbsr+dQvF1FWVcf0k/tzy4WjOHlg291uu6J4H1f+4jWKhuXx0LVT\nm72R4tX1pXx+3psH54oM7JnNjCmD+PiUQYw+joHvvVV1fPmxpby4toQrJg3kex+fSCTqfPaBNyh4\n9QXm/vn7pDdEoD5uNnxmZuz1+ONw6aWHfF5dJMpZ33+BUwb35NfXnH7M8cixSdZtrlOBDe6+KQhi\nPjADSJgggKuAbwbvLwGec/eyYN/ngOnAo20dZKQhyq2PvkN9JMo9n56i5HACy85M5/tXnsKQ3t24\ne+FaenbNZPaMk9vkun5JRS2fn/cmaUZoE78mDu7Jdz46ga8+vpwfPPMud3x4/GFtlheX88WHlzCy\noAcPXjuVN94r44m3i5n78ibue2kjEwblHqz91T0rg25Z6Qff9+yWSV63LPK6ZZLXPYucLhksK97H\nTb99m5KKWr710QnMmjb0YH/99vx8Mr/yPdLrDhwebH197PWJT8Dy5TBy5MFNT6/cQWllLVefNbzN\n+0iOTZgJYhCwNW65GJiWqKGZDQMKgReOsO9htzOY2Q3ADQBDhw49riCL99awdW813/34RN1SJwDc\ndP4o9tfU88uXN5HXLZMvX3xSqz6vui7CFx5cTGlFHfNvOCPUWcGfLBrCim37+NUr7zFxcC+uiJsr\ns6mkkmt+s5i87lk8eO1U+uVmc8WkgVwxaSAlFbU8uWw7f162nQXLtlNd10Bd5MjPDmmsnTWgZ1d+\n/69nMmlIr0O259z7M9wbjhxwfT3MmQP33HNw1YOvbaYwvzvnjMo/9g6QNpUqg9Qzgcf9qL9Nh3L3\nucBciF1iOp4DD8/vzgtfOe+IRe3kxHP7pWMpr67nZy9sILdrJl84Z8RxfU5D1Ln10aWs2LaPX362\n6LAv0TB848PjWb19P//5+HJG9+3BuAG57Nx3gM8+EDuDefi6afTLPXTSXUFOF649u5Brzy48uC7S\nEKW6voGauoaDBRPLq+soq4r9u7e6DsP4wjmF9OqWdXggjzyC1R9eZPEQ9fXw8MMHE8SK4n28vaWc\nb14+XiVRUkCY34rbgCFxy4ODdYnMBG5qsu95TfZ9qQ1jO4SSgzRlZnz34xOpqK3n239dQ8+umXyy\naMjRd4zj7sx+chXPr9nF7Bknc1E73B0FsbLsP591Kh/52at88eG3ePi6qVz/0BL21dQz/4YzKGzh\nrbwZ6WnkpqeRm53JcUVeWXnM7R5ctJluWelcedrg4zmitLEw72JaDIw2s0IzyyKWBBY0bWRmY4E8\nYFHc6oXAxWaWZ2Z5wMXBOpF2k55mzPmXyZwzOp///MNyFq7aeUz7P/Dqezy46H2uP6eQz505PJwg\nm9E3J5v7Zp3Gjn01XPSTl9lcWs3cz53WvkXverTwkm3QrqyqjgXLtvPxUweRm50ZYmDSUqElCHeP\nADcT+2JfAzzm7qvMbLaZXRHXdCYw3+NupwoGp79FLMksBmY3DliLtKcuGen8YtZpTBrSi1t+9w73\nv7LpqM/1dncef6uYb/91DZdN7M/XLx3XTtEe6rRhecyeMYG0NPjZVZM5a2Q7X9OfNSt2p9KRZGbC\nZz8LwP8t3kpdJMrV7ZxMpXmq5irSAuXVdXzlsWUHn/3x3Y9PZHKC8YS33t/LD55+lzc3l3H68Dwe\nvm5a0u+Mq2+IJucxrxs3wimnQHV18226dYPly4kML+SDd7/EsD7d+N31Z7RfjHLE21xVS0KkBXp1\ny+L+q4u47zOnsqeqlo/9/B98888r2X8gNgi7YXcFNzy0hCvve41NpVV866MT+N31ZyQ9OQDJewb4\nyJGxeQ7duh12JlGXlk5tl+zY9pEj+du7u9lWXtPul+LkyDQ6K9JCZsalEwdw9uh8fvzsOh5ctJmn\nV+7kjBF9+Mvy7XTLyuArF43h2rMLdeNDo0svjc1zmDMndrdSZSX06MHqD36EWwecz12Fp3IB8NCi\nzQzq1ZUPjeub7Iglji4xiRynZVvL+a8nVrBuVwWfPWM4N50/MmWfL5FqaiMNXP6/r7K/JsK9nzmV\nK+97ja9NP4kbzxuV7NBOOHqinEhIolGnPhqlS0byLyV1NEu3lvPxn/+D7lkZ1DZEef3rF9K7e4L5\nFBIqjUGIhCQtzZQcjtPkIb24/pwRVNRGuGLSQCWHFKQLpSKSNF+6aAyZ6WlcNe34SuVIuJQgRCRp\nsjPTue2S1tW6kvDoEpOIiCSkBCEiIgkpQYiISEJKECIikpAShIiIJKQEISIiCSlBiIhIQkoQIiKS\nUKepxWRmJUA5sK+ZJj2b2ZZofdN1R1rOB0qPNd4WaC7e1rQ/UpuO1D/H2jct3act+udo/dV0e2fu\nn1T83WkurrbYp6P2zzB3L0i4xd07zQuYe6zbEq1vuu5Iy8CS9v5Zjrd9Z+mfY+2b9uyfo/VXgvad\ntn9S8XdH/XNsr852ienJ49iWaH3TdUdbDsOxHqMl7TtL/xzP57dX/xytv1Lxd6el+xxr/6Ti787x\nHuNE6p+DOs0lpmQxsyXeTKlcUf8cjfqneeqbI2uP/ulsZxDJMDfZAaQ49c+RqX+ap745stD7R2cQ\nIiKSkM4gREQkISUIERFJSAkijpn92sx2m9nK49j3NDNbYWYbzOxnZmZx224xs3fNbJWZ/bBto24/\nYfSPmd1lZtvMbGnwuqztIw9fWL87wfavmJmbWX7bRdy+Qvrd+ZaZLQ9+b541s4FtH3n7CKl/7g6+\nd5ab2RNm1utYP1sJ4lDzgOnHue99wPXA6OA1HcDMzgdmAJPc/WTgR60PM2nm0cb9E5jj7pOD11Ot\nCzFp5hFC35jZEOBiYEsr40u2ebR9/9zt7qe4+2TgL8CdrQ0yiebR9v3zHDDB3U8B1gFfP9YPVoKI\n4+4vA2Xx68xspJk9Y2ZvmdkrZja26X5mNgDIdffXPTbq/xDw0WDzvwHfd/fa4Bi7w/0pwhNS/3QK\nIfbNHOBrQIe+mySM/nH3/XFNu9OB+yik/nnW3SNB09eBwccalxLE0c0FbnH304DbgJ8naDMIKI5b\nLg7WAYwBzjGzN8zs72Z2eqjRtr/W9g/AzcFp8K/NLC+8UNtdq/rGzGYA29x9WdiBJkmrf3fM7Dtm\nthX4DB37DCKRtvi/1eha4OljDSDjWHc4kZhZD+As4Pdxl4W7HOPHZAC9gTOA04HHzGyEd4L7i9uo\nf+4DvkXsr79vAT8m9svcobW2b8ysG/BfxC4vdTpt9LuDu98B3GFmXwduBr7ZZkEmUVv1T/BZdwAR\n4LfHuq8SxJGlAeXBNc6DzCwdeCtYXEDsSy7+9G0wsC14Xwz8MUgIb5pZlFiRrZIwA28nre4fd98V\nt9+viF1L7gxa2zcjgUJgWfAFMRh428ymuvvOkGNvD23xfyveb4Gn6CQJgjbqHzO7BvgIcOFx/VEa\ndrGnjvYChgMr45ZfAz4ZvDdig82J9nuT2FmCETuVuyxY/6/A7OD9GGArwQTFjvgKoX8GxLX5EjA/\n2T9jqvRNkzabgfxk/4yp1D/A6Lg2twCPJ/tnTLH+mQ6sBgqOO6Zkd0oqvYBHgR1APbG//K8j9lfc\nM8CyoLPvbGbfImAlsBG4pzEJAFnAI8G2t4ELkv1zplj/PAysAJYT+4toQHv9PKneN03adOgEEdLv\nzh+C9cuJFbEblOyfM8X6ZwOxP0iXBq9fHGtcKrUhIiIJ6S4mERFJSAlCREQSUoIQEZGElCBERCQh\nJQgREUlICUI6NTOrbOfj3W9m49vosxqCSqUrzezJo1XjNLNeZnZjWxxbBPREOenkzKzS3Xu04edl\n+D8LoIUqPnYzexBY5+7fOUL74cBf3H1Ce8QnnZ/OIOSEY2YFZvYHM1scvD4QrJ9qZovM7B0ze83M\nTgrWX2NmC8zsBeBvZnaemb1kZo8H9fZ/G1eD/yUzKwreVwbF5JaZ2etm1i9YPzJYXmFm327hWc4i\n/lnEr4eZ/c3M3g4+Y0bQ5vvAyOCs4+6g7VeDn3G5mf13G3ajnACUIORE9FNiz6A4HbgSuD9Y/y5w\njrtPIVYZ9Ltx+5wKfMLdPxgsTwH+AxgPjAA+kOA43YHX3X0S8DKxmv2Nx/+pu0/k0EqcCQX1dy4k\nNtMc4ADwMXc/FTgf+HGQoG4HNnrsuRpfNbOLiT0fYCowGTjNzM492vFEGqlYn5yIPgSMj6uSmRtU\nz+wJPGhmo4lVl82M2+c5d4+v1/+muxcDmNlSYnV0Xm1ynDr+WXzwLeCi4P2Z/POZD7+j+YdIdQ0+\nexCwhtgDYCBWc+e7wZd9NNjeL8H+Fwevd4LlHsQSxsvNHE/kEEoQciJKA85w9wPxK83sHuBFd/9Y\ncD3/pbjNVU0+ozbufQOJ/y/V+z8H+ZprcyQ17j45KP29ELgJ+BmxZx8UAKe5e72ZbQayE+xvwPfc\n/ZfHeFwRQJeY5MT0LLHqnwCYWWNJ5Z78s1TyNSEe/3Vil7YAZh6tsbtXA7cCXzGzDGJx7g6Sw/nA\nsKBpBZATt+tC4Nrg7AgzG2RmfdvoZ5ATgBKEdHbdzKw47vVlYl+2RcHA7WpiJdkBfgh8z8zeIdyz\n6/8Avmxmy4FRwL6j7eDu7xCrWnoVsWcfFJnZCuBzxMZOcPc9wD+C22LvdvdniV3CWhS0fZxDE4jI\nEek2V5F2FlwyqnF3N7OZwFXuPuNo+4m0N41BiLS/04B7gjuPyukEj1iVzklnECIikpDGIEREJCEl\nCBERSUgJQkREElKCEBGRhJQgREQkof8P0OfkX/garr8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWE7Fb1i47_g",
        "colab_type": "text"
      },
      "source": [
        "We will pick a value a bit before the minimum, where the loss still improves. Here 2x10^-3 seems to be a good value.\n",
        "\n",
        "Next we will use ``fit_one_cycle`` with the chosen learning rate as the maximum learning rate. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tpavMfe47_g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "6b2b2306-a876-4508-efc9-ce244a3ec67e"
      },
      "source": [
        "learner.fit_one_cycle(1,max_lr=2e-03,moms=(0.8,0.7))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>error_rate</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.524871</td>\n",
              "      <td>0.439260</td>\n",
              "      <td>0.823916</td>\n",
              "      <td>0.176084</td>\n",
              "      <td>00:24</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXxU1f3/8dcnk40skBUCCSFh39ew\nCKiASsEFsIiAWita0Spatdriz9a9LWrVVov7l2rdECkqVQQXNhWQhD1AgICBJEASAlnJnvP7YyZx\nCFkGSDLh8nk+HjyYe++Zmc9cwjt3zj33XDHGoJRS6vzn4e4ClFJKNQ4NdKWUsggNdKWUsggNdKWU\nsggNdKWUsghPd71xWFiYiYmJcdfb1+mnY4UUlJTTq31rPD3E3eUopdQpNm3adMwYE17bNrcFekxM\nDAkJCWf13BOFpfz5s0Q6hfrx4PgeiDRe8F763CoOZp+kZ+cQXpo5iLaBvo322kopda5E5GBd287L\nLpdvkzL5fPsR5q/az96MgkZ9bS+bfZdsOHCcez7YQmFJOZl5xdXbddy+UqqlctsR+rnYfOhE9eNV\nezLpERF4zq9ZUl6BTYT0E0XcNjqWzuH+PPJJInFPf0NRWQVX9G6Hr5eNXYdzWXLXKNq08jrn91RK\nqcZ0fgb6wRNc3C2MrPwSlmxOY8bQjgT5eZ/Ta/b403KGxgRTVFZBZFArbhgWTUlZJd8nHyPE35vF\nm9Kq277w1R6emNz3XD/GeSEjr5h2rbXbSanzgdsC/Xhh6Vk9r7isgr0Z+Yzv3Y4+kW2454MtDHzy\nayJa+/Leb4bTtW3AGb9mSXkFAPEp9iP/qOBWiAi3jo7l1tGxANw2OpZgP2/+8c1ePkpIZc64boQH\n+pzVZ2hKL3y1h/BAH24Y3onyykp8PG1n/Vqr9mQy69/xXNo9nDsv7cJFXUIBOJR9klbethb5+ZW1\nlZWVkZaWRnFxccONz3O+vr5ERUXh5eV6b4DbAv1YQQkVlYbtaTkMig4+o+dVGogMbsUv+kTwrxsG\nMfvdTRzNK+bxpTt597ZhZ3yS9GjuqT8cUcF+p7Xp1b41ADeN6MTC+FSG/uUbBkUH8dbNcYQGnBps\nFZWGg9mFdA4/818uZyu7oIQXv9nLexsOAfDk57sI9PWie7sAxveO4NbRsXy48RDBft5M6BtR5+sY\nY6r336db0gFYszeLdfuP8fvxPQj09eSRTxJp19qHDQ9f1qgnpJVqSFpaGoGBgcTExFj6Z88YQ3Z2\nNmlpacTGxrr8PLedFK008OZ3B7j2lXVsOJDt8vOOFdiP7MMcITq+TwSb/nQ5T0zqw/fJx1i24+gZ\n15KeU1T9OCzAh5719Mn3jWzDA1d05zejY9l1OI8HFm07ZbsxhnsXbmHc82v4ft+xM67lbL2zLqU6\nzGcOi+bG4Z04XljKhgPHefLzXcTM/YKHl+zgvo+28Nqa/XwUf+i0E7ypx08y5OlveP6rPRSWlLN6\nTxbje7fjttGxlFUY5n2ZxCOfJAKQkVfC6r1Zzfb5lAIoLi4mNDTU0mEOICKEhoae8TcRtx2hVxrD\n/kz7CJWfjhUyonPoaW0Wb0rjiaU7if/T5fh62bsOsgtKAE45Kg4N8OGmEZ1YlJDKU5/vYkyPcPx9\nXP9oh3PsO23WqBjuGtMVjwbGn997WTcA2rb24a/LktiWmsOAjkEA7DycxxfbjwDw2/c2cf8V3Zk1\n6syOJpYnHiG/uJxpcR1dfk7i4TwA5t8wmKv6twegrKKSI7nFHC8spbyykpMlFWTllzDvyyTA/k1k\nVNew6tfYcCCb44WlvLwymZdXJgP2byTDO4ewKD6VgdFB/H58DzoE+TLlXz/w6qr9jO3R1uUalWoM\nVg/zKmfzOd0a6FWhe7K0gr98sYvxfSIYGhNS3eZfK/eRX1LO8sSjTBkUCdi7XADCAk49CWrzEJ6c\n3Jepr67jpZX7eHhiL5drOew4Qv/jhJ7VvzhcMXNYNC99m8xL3+7jrV/HISLV3zb+OWMgC35I4cnP\nd3Ewu5DHJ/Vx6R+ootJw53ubAWjX2pcBHYPIKyqjY8jp3UBg/0bw0OLtrEzKZOrgqOowB/jLtf1O\na1tQUk5peSVXv/w9D328jR4RgRhH252H82jlZePZ6/pzz4dbCPDx5OJuYYgI388dh7+3DU/HsM7b\nL+nME//bxbr9xxjZJQyllPu5rcvFGKrDM6+ojDe/+4nliad2l8SE+QPwUXxq9bqaXS7OhnQKZtqQ\nKF5fc4DHPkuksrLuMePFZRX8+dNEViZlcCS3iLAA7zMKc4BAXy9+d1k3vk3K5OME+yiYDQey6Rzm\nz+SBkXx610huGx3LO+sPct1r61m9JxNjDMYYlm47zN+W7eamt35kX0Y+J0vLWbI5jS7/b1n169+8\nYCMj/votFz+7ioufXcmrq/djjOGZ5Ulc9Ldv+WxrOkfziqtH4PTp0LreekWEQF8vQgN8eHnmICKD\nW5GRV0L8T8cZNW8lb69LoWf7QK4Z0IH3fzOcz+aMqv4l1KaVV3WYg/2XWYc2vvxtWVK9+1kpK8nJ\nyeGVV1454+ddeeWV5OTkNEFFp3LrsMWqno0Mx4U7uUVlp2zPyLMfjW9MOc6JwlKC/b3Jyi8h0Mez\nzvB9+MpebEvL4Z31B5kW15G+kW1Oa2OM4Q+Lt7N022He3WC/6Gp4bMhp7Vxx2+hYViZl8vj/drL7\naB5r9mYxfai9q0REeOTKXsSG+fPKqmRu+Xc8/aPa4CHC1tSf/3GveHHtKa/ZrrUPl/dqx7IdRzhx\nsoy2gT6UlFXyzPIkNh86wde7MgD4w+LtTB0SBUCPdoGnHJ03JC4mhI/vHAnAxp+O88LXe9hw4Did\nHN8EnLtiauPrZePBX/TggUXb+GxbOtcOinL5vZU6X1UF+l133XXK+vLycjw9647TZcuW1bmtMbk1\n0MsqKgE46gj0vBqBfjS3iH6RbdiRnsuqPZn8cnAUxwpKCKtnuFyIvzfv3DqMi/62kg0Hsk8L9MpK\nw9vrUli67TD3XtaN7/dlsflQDg9f6XoXjTMPD+HF6QO5471N/PuHFNq19qnuY6/aftOITlwf15El\nm9N4e10KeUVl/OXavozp0Zb3Nhzk1dX7mTYkilFdw5jQNwIvmwc2D+Ev1/ajoKScAB9PSsorGPDE\nV3y9K4OLOofy0sxB3LxgIx/8aD8R+tmcUWf8DaPKsNgQPrx9BF/sOHJGI46mDIzk7XUp/H7RNp75\ncg8vXD+AkQ38IlDqfDZ37lz279/PwIED8fLywtfXl+DgYJKSkti7dy9TpkwhNTWV4uJifve73zF7\n9mzg56lOCgoKmDhxIqNHj2bdunVERkby2Wef0apVq0apz6VAF5EJwD8BG/CWMWZeje0vAmMdi35A\nW2NMUEOvW3VEXjVs0PkIvbisghMny7h1VDuOF5byzroUrh0UaQ/0gPovImrfphWdQv348afj3DSi\nE75eNiorDbPejmeNY2TGxd3CuP/ybtw9tgtHc4vpFOrvyq6oVUQbXz69aySHjp8kwMfztGGMAN6e\nHswYFs2MYdGnrH9ofA+mDYmqc4hjgOM8g4+njU4h/uzJyOe20bGEB/rwzqyhDPvrt3jZ5KzDvIqI\ncHX/Dmf0HA8P4a1fx/Hyt8m8u+Egr609wMiuYRSXVfBRfCrXDOhAiP+5XfClVF2e+N9OdjkGAzSW\n3h1a89g1fercPm/ePBITE9m6dSurV6/mqquuIjExsXpo4YIFCwgJCaGoqIihQ4cydepUQkNPHfCx\nb98+PvzwQ958802uv/56/vvf/3LTTTc1Sv0NBrqI2ID5wBVAGhAvIkuNMbuq2hhj7ndqfw8wyJU3\nrw70Wrpcqrph2ge14r7Lu/HQ4u38b/sRkjMLGO3CUeCwmBCWbEmn55+X88AV3RnYMag6zAEeuKI7\nImIPynMI8yoiclav4+EhLo9Xf/76ASxKSGVsT/vIkratffn8ntHY3DgrZNtAX56a0pfQAG/+8c0+\nnlmeROrxk3y+/QjvrE9h4e0jaKtXmiqLGjZs2CnjxF966SU++eQTAFJTU9m3b99pgR4bG8vAgQMB\nGDJkCCkpKY1WjytH6MOAZGPMAQARWQhMBnbV0X4m8Jgrb55XVA5Azskyx/LPgV41lLB9G19GdA7l\nre9+4t4PtwDUe2FMlX5RbfjYcbLwha/3AvbumO//OJaMvBJiw849xJtb38g2p3Uh1XaOwB1mjYrl\n8+1HeHX1/up1R3KKmTL/Bxb/diQdghrnK6VSVeo7km4u/v4/58jq1av55ptvWL9+PX5+fowZM6bW\nceQ+Pj9/g7fZbBQVFZ3W5my5EuiRQKrTchowvLaGItIJiAVW1rF9NjAbwDui62knQZ2X41OOIwI9\nIgKxeQiPXdObG976kUBfT8a4MPa5T4efg65fZBsGdGzD5b3a4eftSWzYeTmFTYvWppUXH80ewfb0\nXA5kFXJZz7YUlJQz840NPLxkB+/cOszdJSp1zgIDA8nPz691W25uLsHBwfj5+ZGUlMSGDRuaubrG\nPyk6A1hsjKmobaMx5g3gDQCf9t1MzUAvLK2grKISL5sHK5My6R8VVD08cWTXMOIfuZyi0gqX+ot7\nt/95CN//7hl9tp9HnYHQAB/G9mjL2B4/r/vt2C48u3wPOw/nnvJLVqnzUWhoKKNGjaJv3760atWK\ndu3aVW+bMGECr732Gr169aJHjx6MGDGi2etzJdDTAedLFqMc62ozA7jb1TevGehg73Z5/8dDbE3N\n4YErup+y7Uwmg2rlbWNwdBCDz2DUhmp8Nw7vxEvf7uPDjYd4ekq/hp+gVAv3wQcf1Lrex8eHL7/8\nstZtVf3kYWFhJCYmVq9/8MEHG7U2VwI9HugmIrHYg3wGcEPNRiLSEwgG1rv65gUl5aet+z75GC98\nvZer+rXnNxe7PilNbZbcNeqcnq/OXZtWXozvHcHn248wa1QsNpHqC8aUUo2rwStFjTHlwBxgBbAb\nWGSM2SkiT4rIJKemM4CF5hxv6fOXL3YT4u/NvKn98PPWvm4ruG5IFDkny7js+TWM+ftqvtp5VK8u\nVaoJuJSYxphlwLIa6x6tsfz42RQQGdTqlNkOM/NLuOOSzgT66h2BrOLibmEMiGrDtrRcAGa/u4nB\n0UHMGhXLoOigWqcrVkqdObffUzTY/+fgjnZcdn7dEL2M3EpEhHlT+/Onq3qx5+kJPHddf5KO5nPP\nh1t4eMkOd5enlGW4rU/DwzHpk5fNgzd+NQQvTw9GxIZy8Hgh3dqd+z1CVcvSq33r6puETIvryKDo\nIK5/fQM//nSc8orKUyb+UkqdHbf9L6q6uNHL5sH4PhGM7dGWVt42ekbUP2OgsoaubQN57JrelJZX\nMuqZlezNqH1sr1LKdW4MdHuie+uR2QWrau77jLwSZv07npOlp496Uup8FhBgn9bj8OHDXHfddbW2\nGTNmDAkJCY3yfm7rcqmaf8TbUwP9QtUhqBXv3DqM/OIy5nywhWeX7yEjr5i7x3ZtMVMaKNUYOnTo\nwOLFi5v8fVpAH/qFcTspVbtLu4cD9htSv70uBYADWYV8cPtwUrILuffDrdw1tgs3Du9U52sUl1Uw\n880N9OnQmku7t6VNKy96d2hdPVOlUo1l7ty5dOzYkbvvtl8/+fjjj+Pp6cmqVas4ceIEZWVlPP30\n00yePPmU56WkpHD11VeTmJhIUVERs2bNYtu2bfTs2bPZ53JpElU9LV7a5aKAl2cOZv6qZI7kFvPf\nzWkMefqb6m2PfJJIsJ83V/Y79QYeBSXlvPTtPlbvyWRvRgFbDuVU3yi7S7g/7/9mBBFtdKZHy/py\nLhxt5FFSEf1g4rw6N0+fPp377ruvOtAXLVrEihUruPfee2ndujXHjh1jxIgRTJo0qc5bTr766qv4\n+fmxe/dutm/fzuDBgxutfLcfoWsfugL7VA0P/sI+CcyIziEs3pRGYnoud43tyje7M5j73+0Mjw1h\ny6EcXlmdTEWlobC0gmTHjcYBNv/5ClKyC0k6ks9fl+1m2uvr+N+c0QT51T4ne15xGbkn675fq1I1\nDRo0iMzMTA4fPkxWVhbBwcFERERw//33s3btWjw8PEhPTycjI4OIiNpnhV27di333nsvAP3796d/\n//6NVp/7Al370FUdpsV1ZFpcRyorDR4ewi/6tGPCP75jyis/kHq8iOgQP8oqKjmSW8ywmBB8vDzo\nF9mGEH9vQvy9GRwdTI+IQKa9to5/fruv1mlWTxSWMvPNDSQdzefmizpx80WdeP6rvQyNCeHW0ec2\n5YRqJvUcSTeladOmsXjxYo4ePcr06dN5//33ycrKYtOmTXh5eRETE1PrtLnNwX1dLk7j0JWqTdUv\n/a5tA/nXDYN5b8NBYkL9mTe1PwePFXLDWz/yq4s6cc2A0++0NKRTMNOHRvPu+oNMG9KR3k430P4h\n+Rg3vvUjYJ/w7T/rD/LplnTyisv5MvEoPSMC9VZ6qk7Tp0/n9ttv59ixY6xZs4ZFixbRtm1bvLy8\nWLVqFQcPHqz3+ZdccgkffPAB48aNIzExke3btzdabe4btujGu+yo88+EvhG895vhvHvbcCKDWlVP\np1xbmFd54Iru+HrZuPKl7/jP+hQOZBWw8afjrHXcueqWkTGsnzuOS7uHk1dczovTB9Ap1I8b/+9H\n3vruAACHc4o4x+mJlMX06dOH/Px8IiMjad++PTfeeCMJCQn069eP//znP/Ts2bPe5//2t7+loKCA\nXr168eijjzJkyJBGq03c9cMa07OfYco8ZgztyLypjdeHpJSz7Wk53Pjmj4QGeFNaXslhx/1rB0cH\nVc/GebK0nJ2H8xgaE0JhSTkPLd7Gsh1HeegXPXhuxR4eubIXt1/S2Z0fQzns3r2bXr3O7obu56Pa\nPq+IbDLGxNXW3m1H6FXj0EvLK91VgroA9I8K4qkpfUnJPlkd5gA9nK5I9vP2rL7Iyd/Hk39MH0Tv\n9q15bsUeAN747gDGGPZm5FNSXuu9W5RqEdx+pWiJBrpqYlf3b891Q6Jo19qHVQ+OYWyPcGYO61hn\ne29PD+4c06V6OSu/hNiHlzH+xbU8/fnu5ihZqbPitpOiVT3oesSjmpqnzYO/TxtQPWrm37Mavr/p\n+N72W4tNGdiB7MJSthzKobSikg83HiK7sISr+nXgqv7tG3gV1RSMMXWO8baSs+kOd1+g6xG6amZn\nciLe18vGjsfH4+tlw0OE4rIKisoqeOSTHWw+mMPXuzIYFB1Eh6BWTVixqsnX15fs7GxCQ0MtHerG\nGLKzs/H1PbML49x4YZH9b+1DVy2V801W/H088ffx5PVfxZF24iRjnlvN62v288Tkvm6s8MITFRVF\nWloaWVlZ7i6lyfn6+hIVdWb3hnBboHt72ijCPhxNqfNJVLAfUwdH8WF8KneN7Uq71jq9QHPx8vIi\nNlYv/KqL206KetmE3U9O4JaRMe4qQamzdtfYLlRUGl5fc8DdpShVza2Xabbytlm6H0xZV6dQf6YM\njOSDjQfJyi9xdzlKAS4GuohMEJE9IpIsInPraHO9iOwSkZ0i8kHjlqlUy3P32C4Ul1Uy9C/f8MBH\nWykq1RFbyr0aDHQRsQHzgYlAb2CmiPSu0aYb8DAwyhjTB7ivCWpVqkXpHB7ALwdFAvDJ1nQufnYl\nr63Zr1MFKLdx5Qh9GJBsjDlgjCkFFgKTa7S5HZhvjDkBYIzJbNwylWqZ/vrLfqx5aAwLbx9Br/at\nmfdlEqv26I+/cg9XAj0SSHVaTnOsc9Yd6C4iP4jIBhGZUNsLichsEUkQkYQLYdiRsj5fLxudQv0Z\n3jmUBbcMJTrEj398s0+P0pVbNNZJUU+gGzAGmAm8KSJBNRsZY94wxsQZY+LCw8Mb6a2Vahm8bB7M\nGdeV7Wm5rEzSo3TV/FwJ9HTAeeKLKMc6Z2nAUmNMmTHmJ2Av9oBX6oJy7aBIPUpXbuNKoMcD3UQk\nVkS8gRnA0hptPsV+dI6IhGHvgtEBuuqCU3WUviM9V/vSVbNrMNCNMeXAHGAFsBtYZIzZKSJPisgk\nR7MVQLaI7AJWAQ8ZY7KbqmilWrJrB0XSrrVP9Q2rlWouLl36b4xZBiyrse5Rp8cGeMDxR6kLmpfN\ng2lDOvLK6mTSc4qIdEzg9cmWNN5df5DxfSK445LOelGdanR6Q0+lmsDM4dF4engw/fX1bDiQTV5x\nGU/8bxebD+Uw78skXvx6r7tLVBakga5UE4gMasXNF3Ui7UQRt7+TwItf7yXnZBmf3zOaqYOjeHlV\nMne/v5nS8kq2peZQXHb6VaZZ+SUkZxZQXFZBZaVh7d4sMvPdczd5dX5w22yLSlnd3Ik9iYsJ4c73\nNvHvH1K4un97+ka24cnJffAQ+HhTGvsy89mbUUBrX0+u7Neev17bDw8PobyikinzfyA9pwg/bxvd\n2gawLS2Xrm0DeO2mwXRtG+juj6daID1CV6qJeNo8mNA3gqcm92Hq4CjmTrTfDd7fx5Nnr+vP0Jhg\n9mYUAFBUVsHC+FQ+3WofEfxtUibpOUX8dkwXooJbsS0tl0HRQSRnFnD5C2vZfSQPYwwHswtrHR6Z\nX1zGM8uTyMgr5kRhKUs2p3Egq6D5PrxyC3HXWNm4uDiTkJDglvdWqiUoKq1gS+oJBkcH423zYMor\nP5CZV8LKBy/l3g+3siM9hx/+OI7cojL2HM1nZNcw1u7N4uYFG5k0oAO92rfmmeVJXNwtjEu7hzNj\nWDQBPvYv3f9auY+/f7WXYD8vCksrKC2vxNNDWHTnRQyODnbzJ1fnQkQ2GWPiatumR+hKuUkrbxsj\nu4TZb3PnITx6dW+O5hUz7u9r+GZ3Blf164CnzYPQAB9Gdg0D4JLu4dxxSWeWbjvMM8uTANhyKIen\nv9jNQx9vA+y/KN7dcBAAA1wfF8WiOy4iNMCbx5fupLxC7xJmVdqHrlQLERcTwj+mD+S+j7YCcFX/\n2u/m9ccJPenVvjWlFZVMGRiJt6cHL3y1h5dWJvPrBRtZs9c+T9KiOy5iWGxI9fP+dFVv7vlwC6+t\n2c+ccXohtxVpoCvVgkwZFMng6GC+S86qs2vEw0OYMujU+fFuu7gza/cdY93+Y/SLbENcTPApYQ5w\nzYAOLN95lJdWJjNpQCTRoX5N9jmUe2gfulIWUlFpsHnUfcHS0dxiLnt+NV6eHlzTvwMDOgZx3ZAz\nuxGxci/tQ1fqAlFfmANEtPFlyV2j6BTqz7sbDvLgx9tYt/9YM1WnmpoGulIXmB4RgXx8x0V8fOdF\nRIf48fCSHXr7PIvQQFfqAuTt6cHQmBDmTe3HweyT/OMbnYrACvSkqFIXsJFdwpgxtCNvfncAb08P\n2rb2ZUBUG/pHnXZ/GnUe0EBX6gL3yFW9OFlawcsrkwHw8fRg7R/G0q61r5srU2dKu1yUusAF+nrx\n0sxBvHbTEO64tDMeItz74RYqKvWOSy1JUWkFPx0rrLeNHqErpQCY0DeCCX0j6BIewB8Wb+e/m9K4\nfmjHhp9oceUVlXy69TAl5RVMHhhZPb1Cc9mamsPWQydYGJ9KUS2zcjrTQFdKnWLakCgWbjzEU1/s\nIjTAm8t6tXN3SS6rrDR4NDB0s6Z3NxzEy0MY1TWMjiGnX2z1+toDPLdiDwDvbzjEJ3ePxMfTVr29\nuKwCXy/bac87Vx/FH+LPn+6k1DFVQ1iAD3+f1p+xf6j7OdrlopQ6hYjwzxmDiAr24/6PtnK8sBSA\nHWm5JGfms+FAtttvgJ17soz0nCJyTpZW17LnaD6DnvqaN9e6fjvj5MwC/vxpInOX7ODiZ1cx9dV1\n5BaVVW9//qs9vPTtPsb0CGf+DYPZdSSPm/9vI2knTlJZaXh3w0H6PLaCu97fdE5z5OQWlfHK6mSe\n/2oP65KPUVxWwVOf76a0opKeEYH8+P8uI/6RyxjTo229r6NXiiqlarU3I5+J//yOG4ZFc/8V3Rn8\n1NfV2y7v1Y47L+1MXExIPa/QeLLyS8jIK6ZvZBtyi8q4ecFGtqXmAHDXmC78YUJP5v53OwvjUwG4\n45LOFJSUc+JkKQ9P7HXKkXd5RSVLtqSTkHKc7Wm5HDhWyPwbBnMwu5BnlicR6OuFzTEn/YmTZVze\nqy1PT+lHRBtf3l2fwrMr9hDo44mfjyfJmQXYPKT6Ct24TsGM6hrGxL4RdGvn2pz1FZWGX766rvrz\neNmE0V3DWLUni9d/NYSLu4Xh5/1zZ0p9V4pqoCul6vTYZ4m8s/5g9XKAjycFJeXVy3+9th83DI92\n+fXKKiqpqDRn3EVxxQtr2JdZwICOQdXB1zncH5sI+zILuP/y7sxfnczkAR2weQgL41PxtnlQWlFJ\n5zB/lt93Cd6eHlRWGm5esJHvk48R6u9NRBtfbr+4c/XcOD8kH2PJ5nQ8BBIP5xHo68n7vxmOl+3n\nzoxtqTk8/r+dlFVUcsvIWK7u354p838g6Wg+vdq3ZveRPFp52Vg3dxxtWnnV2gV0orCUJVvSuWZA\ne5IzCrjhrR95Zmo/xveO4Jp/fU/aiSIm9Ing1ZsGn3bv2XMOdBGZAPwTsAFvGWPm1dh+C/AckO5Y\n9S9jzFv1vaYGulItX+7JMuYt383HCWlEh/rxzf2XUlhaTmJ6Hk9/sYuCknL+9st+DI0JOSX0AI4V\nlDDvyyRKyiv52y/7sS01hwcWbcUY+L9fD6VfVBsKHb8c/LxtZBWU0Dbw9KGSmXnFDPvrt9XLs0bF\nEOjrxX2XdaO0opKLn11FVn4JncP8+fjOiwgN8GHDgWzCAnxIPX6SWW/H4+9tY1pcR9Jzivh6VwaP\nXt2bWaNiGu1G3Vn5JRSVVhAd6kd8ynGmvbae7u0CSDl2ks7h/lw3JApj7JOvpWQXcs8HWziaV4yH\nQNVgot1PTqCVt4384jJW7MxgfJ92tPb1Ou29zinQRcQG7AWuANKAeGCmMWaXU5tbgDhjzBxXd4AG\nulLnj7KKSorLKgh0Cph31qXw2NKdAAT6ejIgKoj7r+jO9/uO8W1SBtvTck97nQ5tfCksrSDU35vl\n913CpH99T87JMq4dHMmrq/fTNtCHuJhg7h7bleTMAqKC/bjutXUYA619Pbm4u70v29kPycdYsfMo\n913enRB/71O2GWOY+uo6th+MhnMAAA6ESURBVKTm4GXzINTfm8t6teWpyX0bLcxrc/f7m/lixxFC\n/L0pKaugsMbUCoG+nvzpql5sOZTDmr1ZXObo1nHFuQb6RcDjxphfOJYfBjDG/M2pzS1ooCt1QTlR\nWMpDi7fTpa0/+cXlrErK5EjuqTexfuCK7gzsGMSmgycoLqvg1tGx7Dqcx6y344nrFEzCwRMNvk+g\n45Z9E/u1xxhzxkGcX1xGeYUhyM+rSUPcWWWlYe2+LPpHBXGsoITU4ycpKqvg8aW7uGVkJ64Z0IFO\nof7V7c/kc51roF8HTDDG/Max/CtguHN4OwL9b0AW9qP5+40xqbW81mxgNkB0dPSQgwcP1myilDpP\nZeYXc8uCeFKyC1n5+zHkFpXRrW1ArX3If1y8nY8S7P3cvxxsH9s9onMomfkldG8XwMsrk/nl4Eg2\n/nSc8X0iuLR7uBs+UeM7m19INTVHoIcCBcaYEhG5A5hujBlX3+vqEbpS1lNeUUlecflpXR81lVVU\n8t2+LNoG+tI3sk0zVWcN9QW6KxcWpQPOl4tF8fPJTwCMMdlOi28Bz55pkUqp85+nzaPBMAfwsnkw\nruf5c8HS+cKVC4vigW4iEisi3sAMYKlzAxFp77Q4CdjdeCUqpZRyRYNH6MaYchGZA6zAPmxxgTFm\np4g8CSQYY5YC94rIJKAcOA7c0oQ1K6WUqoVeWKSUUucRvaeoUkpdADTQlVLKIjTQlVLKIjTQlVLK\nIjTQlVLKIjTQlVLKIjTQlVLKIjTQlVLKIjTQlVLKIjTQlVLKIjTQlVLKIjTQlVLKIjTQlVLKIjTQ\nlVLKIjTQlVLKIjTQlVLKIjTQlVLKIjTQlVLKIjTQlVLKIlwKdBGZICJ7RCRZRObW026qiBgRqfV+\nd0oppZpOg4EuIjZgPjAR6A3MFJHetbQLBH4H/NjYRSqllGqYK0fow4BkY8wBY0wpsBCYXEu7p4Bn\ngOJGrE8ppZSLXAn0SCDVaTnNsa6aiAwGOhpjvmjE2pRSSp2Bcz4pKiIewAvA711oO1tEEkQkISsr\n61zfWimllBNXAj0d6Oi0HOVYVyUQ6AusFpEUYASwtLYTo8aYN4wxccaYuPDw8LOvWiml1GlcCfR4\noJuIxIqINzADWFq10RiTa4wJM8bEGGNigA3AJGNMQpNUrJRSqlYNBroxphyYA6wAdgOLjDE7ReRJ\nEZnU1AUqpZRyjacrjYwxy4BlNdY9WkfbMedellJKqTOlV4oqpZRFaKArpZRFaKArpZRFaKArpZRF\naKArpZRFaKArpZRFaKArpZRFaKArpZRFaKArpZRFaKArpZRFaKArpZRFaKArpZRFaKArpZRFaKAr\npZRFaKArpZRFaKArpZRFaKArpZRFaKArpZRFaKArpZRFaKArpZRFuBToIjJBRPaISLKIzK1l+50i\nskNEtorI9yLSu/FLVUopVZ8GA11EbMB8YCLQG5hZS2B/YIzpZ4wZCDwLvNDolSqllKqXK0fow4Bk\nY8wBY0wpsBCY7NzAGJPntOgPmMYrUSmllCs8XWgTCaQ6LacBw2s2EpG7gQcAb2BcbS8kIrOB2QDR\n0dFnWqtSSql6NNpJUWPMfGNMF+CPwJ/qaPOGMSbOGBMXHh7eWG+tlFIK1wI9HejotBzlWFeXhcCU\ncylKKaXUmXMl0OOBbiISKyLewAxgqXMDEenmtHgVsK/xSlRKKeWKBvvQjTHlIjIHWAHYgAXGmJ0i\n8iSQYIxZCswRkcuBMuAE8OumLFoppdTpXDkpijFmGbCsxrpHnR7/rpHrUkopdYb0SlGllLIIDXSl\nlLIIDXSllLIIDXSllLIIDXSllLIIDXSllLIIDXSllLIIDXSllLIIDXSllLIIDXSllLIIDXSllLII\nDXSllLIIDXSllLIIDXSllLIIDXSllLIIDXSllLIIDXSllLIIDXSllLIIDXSllLIIDXSllLIIlwJd\nRCaIyB4RSRaRubVsf0BEdonIdhH5VkQ6NX6pSiml6tNgoIuIDZgPTAR6AzNFpHeNZluAOGNMf2Ax\n8GxjF6qUUqp+rhyhDwOSjTEHjDGlwEJgsnMDY8wqY8xJx+IGIKpxy1RKKdUQVwI9Ekh1Wk5zrKvL\nbcCXtW0QkdkikiAiCVlZWa5XqZRSqkGNelJURG4C4oDnattujHnDGBNnjIkLDw9vzLdWSqkLnqcL\nbdKBjk7LUY51pxCRy4FHgEuNMSWNU55SSilXuXKEHg90E5FYEfEGZgBLnRuIyCDgdWCSMSaz8ctU\nSinVkAYD3RhTDswBVgC7gUXGmJ0i8qSITHI0ew4IAD4Wka0isrSOl1NKKdVEXOlywRizDFhWY92j\nTo8vb+S6lFJKnSG9UlQppSxCA10ppSxCA10ppSxCA10ppSxCA10ppSxCA10ppSxCA10ppSxCA10p\npSxCA10ppSxCA10ppSxCA10ppSxCA10ppSxCA10ppSxCA10ppSxCA10ppSxCA10ppSxCA10ppSxC\nA10ppSxCA10ppSxCA10ppSzCpUAXkQkiskdEkkVkbi3bLxGRzSJSLiLXNX6ZSimlGtJgoIuIDZgP\nTAR6AzNFpHeNZoeAW4APGrtApZRSrvF0oc0wINkYcwBARBYCk4FdVQ2MMSmObZVNUKNSSikXuNLl\nEgmkOi2nOdadMRGZLSIJIpKQlZV1Ni+hlFKqDs16UtQY84YxJs4YExceHt6cb62UUpbnSqCnAx2d\nlqMc65RSSrUgrgR6PNBNRGJFxBuYASxt2rKUUkqdqQYD3RhTDswBVgC7gUXGmJ0i8qSITAIQkaEi\nkgZMA14XkZ1NWbRSSqnTuTLKBWPMMmBZjXWPOj2Ox94Vo5RSyk30SlGllLIIDXSllLIIDXSllLII\nDXSllLIIDXSllLIIDXSllLIIDXSllLIIDXSllLIIDXSllLIIDXSllLIIDXSllLIIDXSllLIIDXSl\nlLIIDXSllLIIDXSllLIIDXSllLIIDXSllLIIDXSllLIIDXSllLIIDXSllLIIlwJdRCaIyB4RSRaR\nubVs9xGRjxzbfxSRmMYuVCmlVP0aDHQRsQHzgYlAb2CmiPSu0ew24IQxpivwIvBMYxeqlFKqfq4c\noQ8Dko0xB4wxpcBCYHKNNpOBdxyPFwOXiYg0XplKKaUa4ulCm0gg1Wk5DRheVxtjTLmI5AKhwDHn\nRiIyG5jtWCwRkcSzKboZhFGj9hZEazt7Lbk+re3sXIi1daprgyuB3miMMW8AbwCISIIxJq45399V\nWtvZacm1QcuuT2s7O1rbqVzpckkHOjotRznW1dpGRDyBNkB2YxSolFLKNa4EejzQTURiRcQbmAEs\nrdFmKfBrx+PrgJXGGNN4ZSqllGpIg10ujj7xOcAKwAYsMMbsFJEngQRjzFLg/4B3RSQZOI499Bvy\nxjnU3dS0trPTkmuDll2f1nZ2tDYnogfSSillDXqlqFJKWYQGulJKWYRbAr2hqQTcUE+KiOwQka0i\nkuBYFyIiX4vIPsffwc1UywIRyXQeo19XLWL3kmM/bheRwW6o7XERSXfsu60icqXTtocdte0RkV80\ncW0dRWSViOwSkZ0i8jvHerfvu3pqc/u+ExFfEdkoItsctT3hWB/rmMYj2TGth7djfbNN81FPbW+L\nyE9O+22gY32z/n9wvKdNRLaIyOeOZffuN2NMs/7BfmJ1P9AZ8Aa2Ab2bu44aNaUAYTXWPQvMdTye\nCzzTTLVcAgwGEhuqBbgS+BIQYATwoxtqexx4sJa2vR3/tj5ArOPf3NaEtbUHBjseBwJ7HTW4fd/V\nU5vb953j8wc4HnsBPzr2xyJghmP9a8BvHY/vAl5zPJ4BfNSE+62u2t4GrqulfbP+f3C85wPAB8Dn\njmW37jd3HKG7MpVAS+A8ncE7wJTmeFNjzFrsI4VcqWUy8B9jtwEIEpH2zVxbXSYDC40xJcaYn4Bk\n7P/2TVXbEWPMZsfjfGA39iuY3b7v6qmtLs227xyfv8Cx6OX4Y4Bx2KfxgNP3W7NM81FPbXVp1v8P\nIhIFXAW85VgW3Lzf3BHotU0lUN8Pd3MwwFciskns0xMAtDPGHHE8Pgq0c09p9dbSUvblHMdX3AVO\nXVNuq83xdXYQ9iO6FrXvatQGLWDfOboNtgKZwNfYvxHkGGPKa3n/U6b5AKqm+WiW2owxVfvtL479\n9qKI+NSsrZa6m8I/gD8AlY7lUNy83/SkqN1oY8xg7DNK3i0ilzhvNPbvSS1ifGdLqsXhVaALMBA4\nAjzvzmJEJAD4L3CfMSbPeZu7910ttbWIfWeMqTDGDMR+FfgwoKc76qhNzdpEpC/wMPYahwIhwB+b\nuy4RuRrINMZsau73ro87At2VqQSalTEm3fF3JvAJ9h/qjKqva46/M91XYZ21uH1fGmMyHP/pKoE3\n+blroNlrExEv7IH5vjFmiWN1i9h3tdXWkvado54cYBVwEfbuiqoLD53f3y3TfDjVNsHRhWWMMSXA\nv3HPfhsFTBKRFOzdxuOAf+Lm/eaOQHdlKoFmIyL+IhJY9RgYDyRy6nQGvwY+c0+FUE8tS4GbHWf3\nRwC5Tt0LzaJGH+W12PddVW0zHGf3Y4FuwMYmrEOwX7G82xjzgtMmt++7umprCftORMJFJMjxuBVw\nBfY+/lXYp/GA0/dbs0zzUUdtSU6/oAV7H7XzfmuWf1NjzMPGmChjTAz2DFtpjLkRd++3pjjT2tAf\n7Gej92Lvq3vEHTU41dIZ+4iCbcDOqnqw9299C+wDvgFCmqmeD7F//S7D3gd3W121YD+bP9+xH3cA\ncW6o7V3He2/H/kPb3qn9I47a9gATm7i20di7U7YDWx1/rmwJ+66e2ty+74D+wBZHDYnAo07/LzZi\nPyH7MeDjWO/rWE52bO/shtpWOvZbIvAeP4+Eadb/D051juHnUS5u3W966b9SSlmEnhRVSimL0EBX\nSimL0EBXSimL0EBXSimL0EBXSimL0EBXSimL0EBXSimL+P+k4dryeSUZKwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OnyijJE47_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.save('first_cycle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXPQ6Stc47_j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_all(seed)\n",
        "learner.load('first_cycle');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iYeyJop47_k",
        "colab_type": "text"
      },
      "source": [
        "We then unfreeze the second group of layers and repeat the operations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts5gkzbN47_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.freeze_to(-2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnPqoj8547_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 1e-5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUX--pph47_p",
        "colab_type": "text"
      },
      "source": [
        "Note here that we use slice to create separate learning rate for each group."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWWGkQTO47_p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "fa2ed640-51b8-4a70-9e6e-da7fd70598f3"
      },
      "source": [
        "learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>error_rate</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.445957</td>\n",
              "      <td>0.403973</td>\n",
              "      <td>0.822602</td>\n",
              "      <td>0.177398</td>\n",
              "      <td>00:29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpeuvhNp47_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.save('second_cycle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvTu25Ky47_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_all(seed)\n",
        "learner.load('second_cycle');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQHhKAaK47_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.freeze_to(-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPzVywDf47_v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "46f0cd3d-482f-453a-d755-8fd09e1579a7"
      },
      "source": [
        "learner.fit_one_cycle(1, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>error_rate</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.423056</td>\n",
              "      <td>0.393605</td>\n",
              "      <td>0.829172</td>\n",
              "      <td>0.170828</td>\n",
              "      <td>00:34</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXxU1f3/8deZJXtCVkI2SAj7vouI\niGIVkEVRC9aFWpfWStW2LliV2v5s3Wr9WnesULUoIi5gUVEEBJUt7GEPELKSQPZ9Pb8/ZjJM9gGS\nTLx8no8HD2bu3Jn55CbznnPPPfdcpbVGCCHET5/J3QUIIYRoGxLoQghhEBLoQghhEBLoQghhEBLo\nQghhEBZ3vXFoaKiOjY1119sLIS4gReVVJOeU0sXbSvdgH3eXc162b99+Wmsd1tRjbgv0ImsQo+5/\nkzduHemuEoQQF4ib/72ZiqQc2x0/T56Y1p+Zw6LcW9Q5UkqdaO4xt3a5fLXvpDvfXghxASipqGbT\n0RzuntCTf9w4lJhgb/64bDc7UvLcXVqbkz50IYSh7c8spFbDRXHB3DAymv/cPoau/p4sWJFIba2x\nTqyUQBdCGNqetAIABkd1AaCLt5VHpvQjMb2QgX9ezf1Ld7qzvDbltj50IYQ4W1VVVaSlpVFeXt7q\nusUV1XhbzfT1qmLRtZHkpB8nJ932WG8PWHxtJNW1GtAk7tuP2aTat/iz5OXlRXR0NFar1eXnSKAL\nIX4y0tLS8Pf3JzY2FqWaD+D80kpScksxm00E1NQS6udJZKB3vXV6VtWQXVRBfmklEYHehPh5Ulur\nOZxVRKifJ6H+nu394zRLa01OTg5paWnExcW5/DzpchFC/GSUl5cTEhLSYpjnllSSllcGQFVNLZ4W\nM90CvBqt52U1ExPkjbfVTFZhBdlF5SRmFFBZU8vp4grcOXGhUoqQkBCX9kScSaALIX5SlFJoralt\nInCLyqtIyyvFx8NMbIgvPh4Wugf7YGqmO0UpRUywDzVac7LgTHhW1tSSllfm1oOmLX1pNUe6XIQQ\nPxlVNbXszyxEAWaTIirQm/T8MuJCfbGaTZwursRqNhEb4ovJpAjwbr3/2ctqJj7Ml5ziSgK8rVhN\nioLyKk4VVVBVU0tcqG+L4VpRVcPJwnK8rGbCG+wJVNfUYjF3XLtZWuhCiJ+E5746SHZhBdU1tVTV\n1FJeVcPRU8WUV9WQV1pJZXUtReVVBPl4NNsib46Ph4WYYB+6eFvx8bQQ0cWbyEBviiuqKa6odqyX\nn5/Pa6+95rhfW6tJzimloKyKrMJyamq1bS8ht5Tckkr2ZxZSUFbF1KlTyc/Pd6mWkopqjp0qpqyy\nuvWVG5BAF0J0eqm5pby2/iga8LCYGBzVhQCvM63vgrIq8korAQj2dX1USEuCfT3wsJhIzS2jrLIG\naBzoBWVVVFTXEORtBmwja7IKK8gtrSQtrxSA7MJyVq1ahZ9/gEv98idySh2vc7aky0UI0el9uC0V\nALOCbgFeKKXoEeJDQVkVlTW1nCwop6yyBj9PCx4Wc5u8p0kpYkN8OX66hGOni+nd1Z/58+dz9OhR\nhg0bhtVqRVms+PkHkpacxKfrt3H9rOvISE+jurKCO39zLzfP/RU5JZXE9Ihlyf/WYq2tZO7s6xg3\n7hIStm4mKiqKFStW4O1tG4FTXVNLdW0tJqUoLK+ivLwKrbXL/ekS6EKI87YjJY8ALwu9uvq36esW\nllfh52Fhf2Yh/br5ExHoTaCPBwB//d9+9mcUAlBWWUMtGm+rGdM5HEysMyAygD9PH+i472U10zPU\nl8NZReSUVPDMM8+QmJjIrl27WPPtWmZMn863P27j4mEDyC+t5M/PvUyXoCDCfUxMmjCOO269ia7+\nvo6WeUFZFcePJvH3l9/i32+9xc2/mMPHH3/MLbfcAtgOxgJEBXpTWF5FVlk1f/3ffh6+uh/eHq1/\nUUmgCyHOS1VNLbNe+xGAWcOj2J9ZyCe/HYePx9nFS1llDSWV1YT62cZ/ZxWWM/n/NvCzAeEkny6h\nX0TzXxZe9rBrj1ODPK1m/L2s5JVU4eXUZXK6uIJBw0YwfGBfAAJ9PPhi6SJWrPgMq9lEamoqR44c\nYcyYi2yvYzZh8rTQIzaOfgOHkFVUzsiRI0lOTna8ZmV1rePnCfSxkuFpZvEPyaTmlvLSnOH4era8\nTSXQhRDnZdPRHMftT3baTsX8eEc6t47t0exzUnNLiQr0rnfw8ncf7GTNgSy+euBS+ob78/hnieSV\nVrEsIQ2AyYO6AWcC1bkl3d6CfT0oLC+hxH6AVGtNWWUNAf5+eFltXybr169n43frSNi6BR8fHyZO\nnEh5eTkmk8JqNtEzzI/SUoW3lychvh7kFFeiUVRXnzn4WRfoHmYTSikCfTz448/68MI3hxn9tzVs\nenRSi3XKQVEhxHn5ev9JfD3MfPLbcXz30ESGxgTy2rokisqrmlx/6/FcJjy/jme/OgjYwvG9Tcms\nOZAFwIMf7WbJlhS+2Z/FPRPjHc+LDfVt95+lOX5eFiwmE9UmT4qKiqiqqaUW6nXvFBQUEBQUhI+P\nDwcPHmTz5s31XsP5yyvMfhZqaVWNY0z96eIK8suqsJhN9aYh+O3lvbhjfByllTVsOnq6xTol0IUQ\n5yUxvZAh0YGM6B5EjxBfFkwbwMnCcgY/+TWvrz9ab2THOz8mM2fhJrSGt78/zqGTRWw4cponVuwD\n4M7xcSSmF/L4Z4lc2juUP/6sD/72bgZ3XpjCpBQhfh6YfQIYNWYsQ4YM4cWnFtQL3smTJ1NdXU3/\n/v2ZP38+Y8eObfb1rGYT/l4WSsqrOV1cSVZhORn5ZZRX1dDFq37HidmkmD+lH74eZjYeaTnQpctF\nXFAqqmtYvj2NqYMiCPL1cHc5P0nfHznNil3p/PqynoQHeHHoZBFzxsQ4Hh/ZI4hFc0fz380nePar\ngwyN6cK4+FC01rzx3VECvK38a85w7lu6k8c+3Ut8mB8AH9w1ltGxQZhNishAb35xUXcsZhP/+dUY\nnlq1n0FRXUg9lu2uH5swP0+Kyqv584tvOpYNjOziuO3p6cmXX37Z5HPr+slDQ0NJTEwEIMTPg7vm\n3U+thlNFtiGKfcL98bQ0bmdbzSYujg9lw5FTLdZo+EDXWvOPrw+x4fBpPr5nHB5NbCzx03Y2w7ru\nXWLrpz2QWchT1w5u58qM6Y3vjvJ90mk+2p7mWNa/W0C9dS7v15WL40O4+OlveefHZMbFh7InrYDM\ngnL+ceNQJvQJY8G0Afxh2W4STuQxdXA3Lo4PAeDRqf3rvdbIHkF8+ttL2v8Ha4XJpIgP86WiupZT\nRRXUan1eMzT6e1np183KqaIKMgvK8LCYHP3xTXngyt4oBYMeaaHGc67mJ2LL8VxeXXeUvekF3Pv+\nDpJPl7j83NTcUrdO0NOR/vTpXm59e0u9s+La2+GsIsfJF+fqpTVHuOrFDWQXlXMgs5B3fkxmX0YB\n20/kMe3ljXyVmOlYt6yyhrUHbf20/92cws/f2MSjn+xFa81bG45x3Ws/sO6g+1qA5+Nf3x5h/aGO\nqf1EbgkWk+Ixp+BtagSKl9XMz0fFsOZANjnFFazcnYHVrLiyf1cAZo2I5v9dO4jrhkfxuyt6d0jt\n50spZZvUK9iHHiFt06cf6GM7ESrIp+U9xkFRXertETTF8C30g5mFjtvf7M/CYlK8fkvr1zFdsSud\n+5fu4u25o5jUP7w9S3Sb7Sfy6NbFi6hAb97fkgLApc+u5c5Le/LbifGOVu+zXx0kNbeUcfGhTB8a\ngb9X25yJd9WLGwA4/vTUc5qICGwH5I5kFzP95e8dZ9Z5W81U1tRSU6uZ9/5ONjwcSGSgt+PKNQ9c\n2ZuPd6SxNTmXrcm5BHhb+PfG49TUan6/bBffPXg5XXza5mfsCOn5Zfzzm8MAJD9zTbu9T3FFNa+u\nSyI1t4xHp/Tjrgk9uahnMIt/SKZfgxZ6nZnDonhzwzE+3ZnO8u1pXD2wm2McOcCtY3u0OBrmQmA1\nmxgQEdAm87EbvoWekluGj4eZt+eOAmyn1bZGa80LX9s+IK0dhKhbv+YndimrmlrNLxdvZcFniWit\n8bSYGNUjiKExgTy/+hCr9p5p2b6+/ij/25PJnz7dy6Lvk8/p/bafyOPvXxxwzF6XkV/meOy19Uc5\nklV01q9ZUlHNgcxCLu0dSl5pFdcMjmDlvEsI8LZQU6u5c7xtHumX1yYBsDfNNpfGnNHd2fjwFRz9\n+1RG9Qjize+OEernwQd3jaWwrIo/frS70e9zyZYTfLozjc5o1Z4Mx+3TxWd/urirbnt7C6+vPwrA\nsJhAAIZEB/Li7GHNdmX2j/CnT7gfT606QEFZFbdc4OHdHIt9mOL5MmygH8gs5B+rD7Fqbwbdg32Y\n1D+cX1/Wk6TsYsdYz+YkZReTkmsL/v/8mMx9HzR/iaqK6houeWYtfR//kgNOewOdmdaarcdzKSqv\nZuOR06TmllFRXcvUwRG8PXc0/SMCeOHrw2itKa+qqffcuqFl2YXl3Pv+DpZsaXwB8pKK6kbTjn64\nLYWFG46xLCGVRd8f57NdtvHKvh5mnl99iKn/2ug4MNSSb/Zncde7CRSVV/HPbw5Tq+FX4+PY/viV\nvPKL4QyJDmTtHyfy+bzx/Glqf+aOi+WDrSl8ve8ke9IKCPXzJDzANmTMbFL88+fDGBcfwms3j+Di\n+BD+PH0gaw5kccu/t/Bj0mme/uIAx04V89inifz+w90cOnn2XzxNKa+qYV+G7dJo20/kUlpZTWpu\nKa+sPcJti7Y6zoBsTV5JJQs3HMfHfmLN57szqK5p+e/7XKTmlrIjJZ97L4/njVtGMCYu2KXnKaWY\nZ+9OCfXz4CIXnyfOjdu7XM7mgNbZeOHrw47wGRJta00MiAigsqaW3y7Zweu3jMDqNK1lSUU1P39z\nEyUV1Qy2r3/d8Cg+3ZnOyt0ZPHR1X2KaGDaVmltKhn0e5aVbU/jLzEFt/rO46kROCT4eFscY14ay\nC8t5atUBTuSWsjvV1lqtrKllyVZbKEcGemE2KW67uAePfrLXPk2p7Xfz8k3DybI//+HluwFYtSeT\nVXsyGRodyCD79Rorq2sZ+OfV3DSmO0/POnPQ8aA9CP/06V7qst7f00LCE1fy3qYTPLXqAF/tO1lv\n97uovIqX1ybh72nBz8vC7ZfE8X9rDrMvo5BpL3/PiZxS/L0sjOwRVK8byNfTwuBoWz2PTO7Hd4dP\n8c9vDlNUXs3o2KB6f2/dQ3x4/64zw8tuu7gHG4+cYs2BbDYds50ws9Q+jwjAlJc2MLx7EB/cNfac\nDrBX19Ry57sJrD9kG63w3A1DeHj5HjwtJqprbXt63lYzd76zjc/mXUJX/zPTse5IyaNbgBeRgd6c\nyCnB28PM2xuPk1dayefzxvPkyn385fP9LP4hmc/njeeVdUe4emA3RsU2H6LlVTXMXriZey7ryeRB\nEY0e11qjNaw/bKt31ohox6gUV00fEkFSVhFXDezWLp91cYbbW+jV7dRVcfRUcaNlI7oHAbZW5jf7\ns+o9ti+jkH0ZhSTnlPL57gxCfD14etZgXrhxKACfO+3WOqvrwokK9GbF7gyq2qF11JSUnNJ63QKf\n7UznsufXc+lza/khqeluonnv72Tl7gxHmIPtBIfFPyQDENHFNkHQVQPCMSl44rNElm6z9a33Cfdn\n7rhY5oyOYVlCGssS0rhhZDSeFhPvb01xvF5dC/aDrSkU2k8sqa6p5dDJIsbEBVOrbWfd3Xt5PG/e\nNhJPi5k7xscRH+bLE58l8qK9LxjgrQ3HWLjhGC98c5i/fL6fdYey2ZdRyPDugZzIKaVfN392Lbiq\n3qx7DXlYTPzmsngOniwiPb+MsT1DWtyuSilenD2MP/ysj2NZQVkVkV28WPfgRHqG+bH9RB47U/Ja\nfJ06Vfa+fMf2ySpyhDnAw8v3AKA13D4ulu8fuZyPfnMxeaVVXPLMWp74LJGK6hoy8suY9dqPjHtm\nLd8dPsVlz69nzN++5c0Nx7iyf1cGRAbw2DW2g5QpuaX86p1tvLXxODe8sYnHPt3LvUt2sDetgJSc\nUvLtsxKC7SSf3an5/Oa/OxrtVVXX1DLxH+uJf+wLnvgskZ6hvvQ8h5N7lFL84aq+ji/9C4mfn+3L\nLyMjgxtuuKHJdSZOnEhCQkKbvJ9Lga6UmqyUOqSUSlJKzW9hveuVUlopNcrVAno/9iXpTv2p56O2\nVjPlpY0s/uE4J3JKuHZYJIBjNy8m2IfdC64iKtCbl9Yc4fPdGVz+j/Xcvngrh+x9uCvnXcLdE3ry\n+LT+eFnNXD8ymnHxIby14Rh5JZWN3rMu0H97eTz5pVVsP9H6Bz0pu4jP7KdIn4sDmYVMeH4dv/tg\nh2NZ3d5IRBdvHl6+p1G3UnFFNQkncpkyqBuX9QnjhRuHsvj20Vw1INyxbkQXW2swxM+TeybGsy+j\nkHc3ncDHw+y4gMDvJp0ZjfDgVX25bngUHyWkkpRt+wLdnXbmy2LIk18z45Xv6fXYl1RU1/LzUTEs\n/uVoVt03noeu7se4+FDA9oF/etYQeoT48Pp3tv70iuoaFv+QzPheoY4v1dsXbyPAy8KiuaN5ZtZg\nXvnFCJcOJNX9HQCtBjrYhpP97ope3Dk+jqeuHcRdl8ax/J5xxIX68vE94zApeH9rCmP+toY/fLjL\n0S118GQhr68/SkFpleOL/YGlu7jo7986RuDc/O8tADx3/RAetwfwZX3COPy3KTw+bQDRQT4MiurC\nu3eM4cr+4by3+QT/3nicxT8cd9Q3d9HWevXeNKY7AENjAtn5xM8A2zGLa4dFMm1IBEu2pLBqbybT\nX/meCc+v4+Kn1zoaNGudRvW8uOZwvdfdl1HIiZxSugf7cHHPEBbeNlJa2OcoMjKS5cuXt/v7tNrl\nopQyA68CPwPSgG1KqZVa6/0N1vMH7ge2nG0Rn+1M597Le53t0xo5drqYA5mF/OVzW2mX9+vK49MG\n1BsO1MXHymPX9OeRj/fwO3vf+PHTJRw/XYKX1cSgyC6OLpo6f54+kGv+tZFnvjzIszcMqfdYSm4p\nvh5mZgyN5MmV+1h3MLvF0Pgh6bTjQ/2/PZk8PLkvfcJdn6GuptY2xA7gi70n2X4ij15d/fh6fxbX\nDY/i2uFRzF20lQ8TUut1X+xKyadW2z78E/qEOZYH+3iwxD7CpW5SJICHru7H/ZP6kJJbgkkpR/dC\nVKA304ZEEB7gRbcuXvzxqr58sTeTKS9t4Ip+XckuqiDY14OxPYP5Yq+t39rfy0JxRTUjewQR10wL\nb0xcMEvuvIipL21kxis/8OfpAyiqqOb2S2KZ1D+cfRmFfLQ9lSdnDCTI14M59hBzhcVs4ts/XsaX\nezPpE+5ad4FSisenDWi0vIu3lcFRXVixy7bH9snOdJJOFXP7JbH8/kNbV9SzXx1kQEQA794xxnFw\nedrL39drqd84KhqlFKNjgxtdvBhgdGwwo2ODmf3mJp5ffQirWTl+v898eZAHruzNgIgACsqq6rV8\ng3w9mNSvKydyS/nbdYPx9bTw4FUlfLIjjX+tTWLWiCh2p+bzj9WHOJFTwgdbU7iyfzhBPlZeXptE\n92AfJvQJIzzAi23JuQAs+/XFja7Ec6GaP38+MTEx3HvvvQA8+eSTWCwW1q1bR15eHlVVVTz11FPM\nnDmz3vOSk5OZNm0aiYmJlJWVcfvtt7N792769etHWVnbNGjBtT70MUCS1voYgFJqKTAT2N9gvf8H\nPAs8dLZFHM1u3D3i7G+r9jOuVyiX9+3a4nq7Ugvq3Y8P86sXUnWmDo6gT7g/N721mXsui+f1746S\nbN+Fb+pKJ327+XPH+Dje3HCMG0ZFM9qpTzIlt5TuIb74e1kZ2zOEFbsyuP/K3k3ONJeWV8rcRVuJ\nDfGhi48Haw5kEeBt4Z8/H9biz+XsiRWJfLIznRtGRvPl3kweXr6bo6dsY+vHxYcwoXcoo3oE8cra\nI2QVlJNbWsnt42K55W3bl8jw7vW/rIbGBPL17ydwqqii0c/uYTE1OR3qK78Y4bgd5u/J/Cn9+dOn\ne1m9z9bqu254FH+ZOZBrh0UxoU8YVrOJmlrdap9zdJAPK+aN54oX1jP/k714mE2Ok00WTB/AgumN\nA9ZV8WF+joNz5+uRyf1Yvj2NaUMj+GxnBit3ZzjCvM7+zEJGPbUGgBdnD2XJ5hRmDo/iic8SuSgu\n2NHSHRoT2Oj1nc0eHcOW47ZgffDqvkQFenOZ0xdyTBPPeePWkVhMyvEesaG+PHBlHyb268qw6EA+\n2JbCY58m8tSqA0zq15WnZw1Ga81H29N4aPkeugV48d3DE1l7MJvuwT6dN8y/nA8n97bta3YbDFOe\nafbh2bNn88ADDzgCfdmyZaxevZr77ruPgIAATp8+zdixY5kxY0azezOvv/46Pj4+HDhwgD179jBi\nxIgm1zsXrgR6FJDqdD8NuMh5BaXUCCBGa71KKdVsoCul7gbuBvDodqZFntREf3edyupa3tp4nLc2\nHm91jG1d33BMsDdWs4leXZtvjfXq6seWRydhMiksZsWCFftanJryvkm9+Xx3Bk98lsiX91/quFDt\n4awihtgPwN0/qTc3vLGJhRuO8cCVfRq9xpr9WVTXahbfPoa4UF/uejeBHU100aw7mE1+WSXXDY+u\nt7ygrIqPt6cxa3gUz8wajKfFxJItKUwfGsn4XiHMGBZpH1XQi18u3sYr62zD9TbbD+795rL4JseQ\n9wn3P6u9hIbmjI7B19NMZKA3aXmlTB0cgafFzFUDuznWcXWMbVyoL+PiQ/ghKYcJfcLOegrWjjCu\nVyjjetm6i8bFh3LXpT1547ujTB8aSU2t5uDJQvqE+7Nil+1YzLXDohy/yxlDI7GaXe+2uHZYFN0C\nvIgL83Uc42iNtYlrWJpMynEM6brhURw+WcTF8SFc7XSgcvaoGD5MSOVkYTnT/vU9R7KLeWRyP5dr\nvRAMHz6c7OxsMjIyOHXqFEFBQXTr1o3f//73bNiwAZPJRHp6OllZWXTr1q3J19iwYQP33XcfAEOG\nDGHIkCFNrncuzvvTopQyAf8EftnaulrrhcBCAM+I3o79z5Za6JkFZ3ZHyqtqWjw1dldqPmN7BrP0\n7otdqPzM7Ge3ju2BSSlHMDfF19PCr8bH8dSqA2QXVRAe4MXx0yWk5ZXx6wk9ARgVG8xlfcJYti2V\n+67o3ajF++3BbOLDfB3dDqN6BPHN/ix+SDpN765+7Mss5Ot9WXxgP8jobTU7Rh4UlFXx6/cSqKiu\nZe64WCxmE09MG8DdE3o2OmOtrlVb59ipEgZEBDB/Svt8OE0mxcxhUQD19l7O1ZPTB/JD0mlmjYxu\nfWU387KaGRzdhVdvPtPKumaI7Xc2fWhko/W7uHDRYmcmk3J8ebQVHw9Lk6Oxnp41mGeuH8x1r/3I\nrtR8Zg6LrDfbYafTQku6Pd14440sX76ckydPMnv2bJYsWcKpU6fYvn07VquV2NhYysvL3VKbKwdF\n06m/ZxdtX1bHHxgErFdKJQNjgZVnc2C0pLKG46dLGPO3NY3G+TofML3znQSmvLSxydcorqhmX0YB\nY84hUJRS3DK2R6O+84YG2/sq68YIf2cfyjXRqSvo+pHRZBSU8+BHu+uNB66ormHL8dx669aN5b35\n31u49Ll13L54myPMARba+8qXbk3htkVb2Xwslz9N7ef44vGymps8/djT6RJcdbvnzfVdd0a9w/35\n5SVxLY5eEW3PZO+muf/K3gT5WJvcyxS2bpelS5eyfPlybrzxRgoKCujatStWq5V169Zx4kTjczOc\nTZgwgffffx+AxMRE9uzZ02a1uRLo24DeSqk4pZQHMAdYWfeg1rpAax2qtY7VWscCm4EZWuuzGofz\n7qZksosqePv7Y/WWp+edCfTvk05zILOQkwWNv/22n8ijVsPodjxxYUCk7fTmuhNCvtx7kl5d/eqN\nT796YDgT+4bxyc70emeZJqYXUlldW68FOywmkPfuGMPCW0cSH+bH5IHdeHvuKH45LpaHru7LjpR8\nZr+5ifmf7GV3aj7dAry4e0K8SyMNHr+mP74eZu6bZOvaathqF6I5l/ftys4FV/2kGgEdaeDAgRQV\nFREVFUVERAQ333wzCQkJDB48mHfffZd+/VreE77nnnsoLi6mf//+LFiwgJEjW5+KxFWtdrlorauV\nUvOA1YAZWKS13qeU+iuQoLVe2fIruCbbfpZgw3HpGfm28A70sZJfahvXvDU5lxn23dmvEjPx9rDw\nwteHMDv1E7YHfy8rPUJ82JNWwImcErYm5/LQ1X3rreNpMfPmrSMZ8ddv+OZAFhP7hrF630m+2W8b\nHjayx5n6lFJc2tvWgv7ZgHBHUE/qH05GfhmvrUtyHBAL8fXg0amud5nceWlP7rzU1hX03UMT3TqX\ntBBGs3fvmYOxoaGhbNq0qcn1iott3cmxsbGOaXO9vb1ZunRpu9TlUh+61voL4IsGyxY0s+7Ecynk\nlH1ipYZzaKTnlxLm70m/bv6OFu+WYznMGBqJ1prf/Nc2FtvDYuK564e0es2983V53668t/mEY4a0\n64ZHNVrH02JmQp8wvtmfxdUDuzlq7N3Vr9mzOBu2uiMDvdn75NWk5JaSU1LByB7nvufRVrPCCSE6\nN7efKQqgFI65UxoG+uGsYroH+9DXPgrD22rmi72ZlFfVOFr1AL8Y053rO+Ag2u+u6IW31cyyhDT6\nhvs3OYYYbKdInyqqcJz5+Oz1g1ly50VNrtsck0kRG+p7XmEuhLhwdIpA7+rvyclCW9eKc6BnFpSx\nKzWfiX3CGNkjCA+LiadnDSavtIqVuzM4Zh9/ffNF3XmwQddHewnx82T6UNsohpauQn5Fv65EBXqz\nKzWfqEBvZo/uTtfOOp5XiJ+QC+UaBefyc7o90C0mVW98bVXNmR9ideJJwDYMbPKgbmx5dBIzh0Uy\nMDKAl9ce4bD9dP17Jsbj185dLc7uGB+HUra9guaYTcoxVejAyKbnihZCnB0vLy9ycnIMH+paa3Jy\ncvDyOrtGoNvP2rCaTYT6nTk133m61pTcMvw8LfS0z+5Wdw3I+yb15tfvbef9LSl4WExEunjCRVvp\n1dWf40+3fiGB2aNjeHVdUr5+csQAAA5VSURBVJuMzRZCQHR0NGlpaZw61fK1NY3Ay8uL6Oiz60Z2\ne6BbzKreFUwKyqoct/NLK5s8EaMuIA9lFdE3vOnT9TuDYF8Pvn/k8g7dexDCyKxWK3Fxce4uo9Ny\ne5eLh9lEsNPV1+umXAXIL6siyLdxoAf7ehBif05LZ3d2BoE+HliaOBVbCCHamtuTxmJW9WZDdG6h\n55VWEujd9IVT66YAGNa95bM7hRDiQuH2QLeaTQQ7tcILy6ocBzwKSqsc470bKq20XZ1+WCuz1Qkh\nxIWiUwS6cx96rYZM+6n9eaWVzQb6yzeNYGLfMMf4dCGEuNB1gkBX9frQva1m7n4vgYrqGgrKqprt\nchnfO5T/3D5G+qeFEMLO7WloNZscfegeZhMvzh5KYnohL605Qq2m2Ra6EEKI+twe6H3C/Qmyh3Zl\nTS2TB0UwvHsgyxJs19Rw7o4RQgjRPLcOkF70y1Fc3DPUcWmyuus9juoRxM4U29WHgqSFLoQQLnFr\noF/RL9xxe8mdFzkugzYsJgiwXeXcuX9dCCFE89wW6A3P7bzE6TJbdRcyjgr0bvUqQkIIIWw65Tnp\nkYHefHj3WAZFdXH54sJCCHGhc18LvZXLqF3UUy6ZJoQQZ8Pto1yEEEK0DbcFugvXORZCCHEW3Bfo\n7npjIYQwKOlyEUIIg3Bjl4u00YUQoi1JC10IIQxC+tCFEMIgZJSLEEIYhHS5CCGEQbgt0H08OuWs\nA0II8ZPltkCPDvJ211sLIYQhSZeLEEIYhAS6EEIYhAS6EEIYhAS6EEIYhAS6EEIYhAS6EEIYhAS6\nEEIYhAS6EEIYhAS6EEIYhAS6EEIYhEuBrpSarJQ6pJRKUkrNb+Lx3yil9iqldimlvldKDWj7UoUQ\nQrSk1UBXSpmBV4EpwADgpiYC+32t9WCt9TDgOeCfbV6pEEKIFrnSQh8DJGmtj2mtK4GlwEznFbTW\nhU53fQHddiUKIYRwhStz2EYBqU7304CLGq6klLoX+APgAVzR1Asppe4G7gbo3r372dYqhBCiBW12\nUFRr/arWOh54BHi8mXUWaq1Haa1HhYWFtdVbCyGEwLVATwdinO5H25c1Zylw7fkUJYQQ4uy5Eujb\ngN5KqTillAcwB1jpvIJSqrfT3WuAI21XohBCCFe02oeuta5WSs0DVgNmYJHWep9S6q9AgtZ6JTBP\nKXUlUAXkAXPbs2ghhBCNuXRhT631F8AXDZYtcLp9fxvXJYQQ4izJmaJCCGEQEuhCCGEQEuhCCGEQ\nEuhCCGEQEuhCCGEQEuhCCGEQEuhCCGEQEuhCCGEQEuhCCGEQEuhCCGEQEuhCCGEQEuhCCGEQEuhC\nCGEQEuhCCGEQEuhCCGEQEuhCCGEQEuhCCGEQEuhCCGEQEuhCCGEQEuhCCGEQEuhCCGEQEuhCCGEQ\nEuhCCGEQEuhCCGEQEuhCCGEQEuhCCGEQEuhCCGEQEuhCCGEQEuhCCGEQEuhCCGEQEuhCCGEQEuhC\nCGEQEuhCCGEQEuhCCGEQEuhCCGEQEuhCCGEQEuhCCGEQEuhCCGEQLgW6UmqyUuqQUipJKTW/icf/\noJTar5Tao5T6VinVo+1LFUII0ZJWA10pZQZeBaYAA4CblFIDGqy2ExiltR4CLAeea+tChRBCtMyV\nFvoYIElrfUxrXQksBWY6r6C1Xqe1LrXf3QxEt22ZQgghWuNKoEcBqU730+zLmnMH8OX5FCWEEOLs\nWdryxZRStwCjgMuaefxu4G6A7t27t+VbCyHEBc+VFno6EON0P9q+rB6l1JXAY8AMrXVFUy+ktV6o\ntR6ltR4VFhZ2LvUKIYRohiuBvg3orZSKU0p5AHOAlc4rKKWGA29iC/Psti9TCCFEa1oNdK11NTAP\nWA0cAJZprfcppf6qlJphX+15wA/4SCm1Sym1spmXE0II0U5c6kPXWn8BfNFg2QKn21e2cV1CCCHO\nkpwpKoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGB\nLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQ\nBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGB\nLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQBiGBLoQQ\nBiGBLoQQBuFSoCulJiulDimlkpRS85t4fIJSaodSqlopdUPblymEEKI1rQa6UsoMvApMAQYANyml\nBjRYLQX4JfB+WxcohBDCNRYX1hkDJGmtjwEopZYCM4H9dStorZPtj9W2Q41CCCFc4EqXSxSQ6nQ/\nzb7srCml7lZKJSilEk6dOnUuLyGEEKIZHXpQVGu9UGs9Sms9KiwsrCPfWgghDM+VQE8HYpzuR9uX\nCSGE6ERcCfRtQG+lVJxSygOYA6xs37KEEEKcrVYDXWtdDcwDVgMHgGVa631Kqb8qpWYAKKVGK6XS\ngBuBN5VS+9qzaCGEEI25MsoFrfUXwBcNli1wur0NW1eMEEIIN5EzRYUQwiAk0IUQwiAk0IUQwiAk\n0IUQwiAk0IUQwiAk0IUQwiAk0IUQwiAk0IUQwiAk0IUQwiAk0IUQwiAk0IUQwiAk0IUQwiAk0IUQ\nwiAk0IUQwiAk0IUQwiAk0IUQwiAk0IUQwiAk0IUQwiAk0IUQwiAk0IUQwiAk0IUQwiAk0IUQwiAk\n0IUQwiAk0IUQwiAk0IUQwiAk0IUQwiAk0IUQwiAk0IUQwiAk0IUQwiAk0IUQwiAk0IUQwiAk0IUQ\nwiAk0IUQwiAk0IUQwiAk0IUQwiAk0IUQwiAk0IUQwiAk0IUQwiBcCnSl1GSl1CGlVJJSan4Tj3sq\npT60P75FKRXb1oUKIYRoWauBrpQyA68CU4ABwE1KqQENVrsDyNNa9wJeBJ5t60KFEEK0zJUW+hgg\nSWt9TGtdCSwFZjZYZybwjv32cmCSUkq1XZlCCCFaY3FhnSgg1el+GnBRc+torauVUgVACHDaeSWl\n1N3A3fa7FUqpxHMpugOE0qD2TkRqO3eduT6p7dxciLX1aO4BVwK9zWitFwILAZRSCVrrUR35/q6S\n2s5NZ64NOnd9Utu5kdrqc6XLJR2IcbofbV/W5DpKKQvQBchpiwKFEEK4xpVA3wb0VkrFKaU8gDnA\nygbrrATm2m/fAKzVWuu2K1MIIURrWu1ysfeJzwNWA2ZgkdZ6n1Lqr0CC1nol8DbwnlIqCcjFFvqt\nWXgedbc3qe3cdObaoHPXJ7WdG6nNiZKGtBBCGIOcKSqEEAYhgS6EEAbhlkBvbSoBN9STrJTaq5Ta\npZRKsC8LVkp9o5Q6Yv8/qINqWaSUynYeo99cLcrmX/btuEcpNcINtT2plEq3b7tdSqmpTo89aq/t\nkFLq6nauLUYptU4ptV8ptU8pdb99udu3XQu1uX3bKaW8lFJblVK77bX9xb48zj6NR5J9Wg8P+/IO\nm+ajhdr+o5Q67rTdhtmXd+jnwf6eZqXUTqXU/+z33bvdtNYd+g/bgdWjQE/AA9gNDOjoOhrUlAyE\nNlj2HDDffns+8GwH1TIBGAEktlYLMBX4ElDAWGCLG2p7EniwiXUH2H+3nkCc/XdubsfaIoAR9tv+\nwGF7DW7fdi3U5vZtZ//5/ey3rcAW+/ZYBsyxL38DuMd++7fAG/bbc4AP23G7NVfbf4Abmli/Qz8P\n9vf8A/A+8D/7fbduN3e00F2ZSqAzcJ7O4B3g2o54U631BmwjhVypZSbwrrbZDAQqpSI6uLbmzASW\naq0rtNbHgSRsv/v2qi1Ta73DfrsIOIDtDGa3b7sWamtOh207+89fbL9rtf/TwBXYpvGAxtutQ6b5\naKG25nTo50EpFQ1cA/zbfl/h5u3mjkBvaiqBlv64O4IGvlZKbVe26QkAwrXWmfbbJ4Fw95TWYi2d\nZVvOs+/iLnLqmnJbbfbd2eHYWnSdats1qA06wbazdxvsArKBb7DtEeRrraubeP9603wAddN8dEht\nWuu67fY3+3Z7USnl2bC2JupuD/8HPAzU2u+H4ObtJgdFbcZrrUdgm1HyXqXUBOcHtW0/qVOM7+xM\ntdi9DsQDw4BM4AV3FqOU8gM+Bh7QWhc6P+bubddEbZ1i22mta7TWw7CdBT4G6OeOOprSsDal1CDg\nUWw1jgaCgUc6ui6l1DQgW2u9vaPfuyXuCHRXphLoUFrrdPv/2cCn2P6os+p21+z/Z7uvwmZrcfu2\n1Fpn2T90tcBbnOka6PDalFJWbIG5RGv9iX1xp9h2TdXWmbadvZ58YB1wMbbuiroTD53f3y3TfDjV\nNtnehaW11hXAYtyz3S4BZiilkrF1G18BvISbt5s7At2VqQQ6jFLKVynlX3cbuApIpP50BnOBFe6p\nEFqoZSVwm/3o/ligwKl7oUM06KO8Dtu2q6ttjv3ofhzQG9jajnUobGcsH9Ba/9PpIbdvu+Zq6wzb\nTikVppQKtN/2Bn6GrY9/HbZpPKDxduuQaT6aqe2g0xe0wtZH7bzdOuR3qrV+VGsdrbWOxZZha7XW\nN+Pu7dYeR1pb+4ftaPRhbH11j7mjBqdaemIbUbAb2FdXD7b+rW+BI8AaILiD6vkA2+53FbY+uDua\nqwXb0fxX7dtxLzDKDbW9Z3/vPdj+aCOc1n/MXtshYEo71zYeW3fKHmCX/d/UzrDtWqjN7dsOGALs\ntNeQCCxw+lxsxXZA9iPA077cy34/yf54TzfUtta+3RKB/3JmJEyHfh6c6pzImVEubt1ucuq/EEIY\nhBwUFUIIg5BAF0IIg5BAF0IIg5BAF0IIg5BAF0IIg5BAF0IIg5BAF0IIg/j/AY5iv5YBKF8AAAAA\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1996_YMY47_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.save('third_cycle')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOfU0FsF47_y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed_all(seed)\n",
        "learner.load('third_cycle');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2IjxWqN47_0",
        "colab_type": "text"
      },
      "source": [
        "Here, we unfreeze all the groups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbjtwQbg47_0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learner.unfreeze()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlhjYMGO47_1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "outputId": "974af84b-c9b2-4121-e11c-96f0993efe15"
      },
      "source": [
        "learner.fit_one_cycle(2, max_lr=slice(lr*0.95**num_groups, lr), moms=(0.8, 0.9))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>error_rate</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.436018</td>\n",
              "      <td>0.392118</td>\n",
              "      <td>0.837057</td>\n",
              "      <td>0.162943</td>\n",
              "      <td>01:36</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.332064</td>\n",
              "      <td>0.400855</td>\n",
              "      <td>0.826544</td>\n",
              "      <td>0.173456</td>\n",
              "      <td>01:39</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkqeJKjiXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnDN5a_m47_3",
        "colab_type": "text"
      },
      "source": [
        "### Creating prediction\n",
        "Now that the model is trained, we want to generate predictions from the test dataset.\n",
        "\n",
        "As specified in *Keita Kurita's* [article](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/), as the function ``get_preds`` does not return elements in order by default, you will have to resort the elements into their correct order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlBCn6aJ47_3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "outputId": "944ef2ab-269b-4caa-927b-b2fe0bc95395"
      },
      "source": [
        "def get_preds_as_nparray(ds_type) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    the get_preds method does not yield the elements in order by default\n",
        "    we borrow the code from the RNNLearner to resort the elements into their correct order\n",
        "    \"\"\"\n",
        "    preds = learner.get_preds(ds_type)[0].detach().cpu().numpy()\n",
        "    sampler = [i for i in databunch.dl(ds_type).sampler]\n",
        "    reverse_sampler = np.argsort(sampler)\n",
        "    return preds[reverse_sampler, :]\n",
        "\n",
        "test_preds = get_preds_as_nparray(DatasetType.Test)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va192DWD47_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_submission = pd.read_csv(\"./sample_submission.csv\")\n",
        "sample_submission['target'] = np.argmax(test_preds,axis=1)\n",
        "sample_submission.to_csv(\"predictions_bert_trans.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujer4Kpq47_9",
        "colab_type": "text"
      },
      "source": [
        "We check the order"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr43V1kA76OC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "outputId": "1d8b9797-9cf4-457a-dd97-0cbc8d1ccd36"
      },
      "source": [
        "sample_submission.target.value_counts()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    1768\n",
              "0    1495\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbKHutCQ47_-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "445ea986-2fa9-4e7a-ef43-77611f79d2d4"
      },
      "source": [
        "test.head()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just happened a terrible car crash</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Heard about #earthquake is different cities, s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>there is a forest fire at spot pond, geese are...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword location                                               text\n",
              "0   0     NaN      NaN                 Just happened a terrible car crash\n",
              "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
              "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
              "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
              "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F03s32HY48AA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "66b296d2-6be1-4cb1-c067-606bfb9cf866"
      },
      "source": [
        "sample_submission.head()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  target\n",
              "0   0       1\n",
              "1   2       1\n",
              "2   3       1\n",
              "3   9       1\n",
              "4  11       1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEY8e5gH48AC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "5634b48e-e0da-4778-f641-be3f546fe7bb"
      },
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "def create_download_link(title = \"Download CSV file\", filename = \"data.csv\"):  \n",
        "    html = '<a href={filename}>{title}</a>'\n",
        "    html = html.format(title=title,filename=filename)\n",
        "    return HTML(html)\n",
        "\n",
        "# create a link to download the dataframe which was saved with .to_csv method\n",
        "create_download_link(filename='predictions.csv')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<a href=predictions.csv>Download CSV file</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TLtyZIn48AE",
        "colab_type": "text"
      },
      "source": [
        "We can now submit our predictions to Kaggle !  In our example, without playing too much with the parameters, we get a score of 0.70059, which leads us to the 5th position on the leaderboard! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spJday4748AE",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this NoteBook, I explain how to combine the ``transformers`` library with the beloved ``fastai`` library. It aims to make you understand where to look and modify both libraries to make them work together. Likely, it allows you to use **Slanted Triangular Learning Rates**, **Discriminate Learning Rate** and even **Gradual Unfreezing**. As a result, without even tunning the parameters, you can obtain rapidly state-of-the-art results.\n",
        "\n",
        "This year, the transformers became an essential tool to NLP. Because of that, I think that pre-trained transformers architectures will be integrated soon to future versions of fastai. Meanwhile, this tutorial is a good starter.\n",
        "\n",
        "I hope you enjoyed this first article and found it useful.¬†\n",
        "Thanks for reading and don't hesitate in leaving questions or suggestions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-nWCbvW48AF",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "* Hugging Face, Transformers GitHub (Nov 2019), [https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)\n",
        "* Fast.ai, Fastai documentation (Nov 2019), [https://docs.fast.ai/text.html](https://docs.fast.ai/text.html)\n",
        "* Jeremy Howard & Sebastian Ruder, Universal Language Model Fine-tuning for Text Classification (May 2018), [https://arxiv.org/abs/1801.06146](https://arxiv.org/abs/1801.06146)\n",
        "* Keita Kurita's article¬†: [A Tutorial to Fine-Tuning BERT with Fast AI](https://mlexplained.com/2019/05/13/a-tutorial-to-fine-tuning-bert-with-fast-ai/)¬†(May 2019)\n",
        "* Dev Sharma's article¬†: [Using RoBERTa with Fastai for NLP](https://medium.com/analytics-vidhya/using-roberta-with-fastai-for-nlp-7ed3fed21f6c) (Sep 2019)"
      ]
    }
  ]
}